2023-04-19 18:23:06,362 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-cased-finetuned-sst2', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-19 18:23:06,363 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-19 18:23:06,363 device: cpu n_gpu: 0
2023-04-19 18:23:09,004 *** Example ***
2023-04-19 18:23:09,004 guid: aug-1
2023-04-19 18:23:09,005 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-04-19 18:23:09,005 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:23:09,005 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:23:09,005 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:23:09,005 label: 0
2023-04-19 18:23:09,005 label_id: 0
2023-04-19 18:23:12,280 Converted and tokenised the training data.
2023-04-19 18:23:12,280 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-19 18:23:12,284 *** Example ***
2023-04-19 18:23:12,284 guid: dev-1
2023-04-19 18:23:12,284 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-19 18:23:12,284 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:23:12,284 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:23:12,284 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:23:12,284 label: 1
2023-04-19 18:23:12,284 label_id: 1
2023-04-19 18:23:12,436 Model config
2023-04-19 18:23:15,688 Loading model models/FineTunedModels/bert-base-cased-finetuned-sst2/pytorch_model.bin
2023-04-19 18:23:15,944 loading model...
2023-04-19 18:23:15,997 done!
2023-04-19 18:23:15,997 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-04-19 18:23:15,997 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2023-04-19 18:23:16,000 Model config
2023-04-19 18:23:18,088 Loading model models/GeneralDistilledModels/TinyBERT_General_6L_768D/pytorch_model.bin
2023-04-19 18:23:18,244 loading model...
2023-04-19 18:23:18,260 done!
2023-04-19 18:23:18,260 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-04-19 18:23:18,260 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2023-04-19 18:23:18,260 ***** Running training *****
2023-04-19 18:23:18,260   Num examples = 1119160
2023-04-19 18:23:18,261   Batch size = 32
2023-04-19 18:23:18,261   Num steps = 34973
2023-04-19 18:23:18,261 n: bert.embeddings.word_embeddings.weight
2023-04-19 18:23:18,261 n: bert.embeddings.position_embeddings.weight
2023-04-19 18:23:18,261 n: bert.embeddings.token_type_embeddings.weight
2023-04-19 18:23:18,261 n: bert.embeddings.LayerNorm.weight
2023-04-19 18:23:18,261 n: bert.embeddings.LayerNorm.bias
2023-04-19 18:23:18,261 n: bert.encoder.layer.0.attention.self.query.weight
2023-04-19 18:23:18,261 n: bert.encoder.layer.0.attention.self.query.bias
2023-04-19 18:23:18,261 n: bert.encoder.layer.0.attention.self.key.weight
2023-04-19 18:23:18,261 n: bert.encoder.layer.0.attention.self.key.bias
2023-04-19 18:23:18,261 n: bert.encoder.layer.0.attention.self.value.weight
2023-04-19 18:23:18,261 n: bert.encoder.layer.0.attention.self.value.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.0.attention.output.dense.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.0.attention.output.dense.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.0.intermediate.dense.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.0.intermediate.dense.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.0.output.dense.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.0.output.dense.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.attention.self.query.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.attention.self.query.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.attention.self.key.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.attention.self.key.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.attention.self.value.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.attention.self.value.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.attention.output.dense.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.attention.output.dense.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.intermediate.dense.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.intermediate.dense.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.output.dense.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.output.dense.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.attention.self.query.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.attention.self.query.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.attention.self.key.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.attention.self.key.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.attention.self.value.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.attention.self.value.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.attention.output.dense.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.attention.output.dense.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.intermediate.dense.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.intermediate.dense.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.output.dense.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.output.dense.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-04-19 18:23:18,262 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-04-19 18:23:18,262 n: bert.encoder.layer.3.attention.self.query.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.attention.self.query.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.attention.self.key.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.attention.self.key.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.attention.self.value.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.attention.self.value.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.attention.output.dense.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.attention.output.dense.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.intermediate.dense.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.intermediate.dense.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.output.dense.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.output.dense.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.attention.self.query.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.attention.self.query.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.attention.self.key.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.attention.self.key.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.attention.self.value.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.attention.self.value.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.attention.output.dense.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.attention.output.dense.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.intermediate.dense.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.intermediate.dense.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.output.dense.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.output.dense.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.output.LayerNorm.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.4.output.LayerNorm.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.5.attention.self.query.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.5.attention.self.query.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.5.attention.self.key.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.5.attention.self.key.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.5.attention.self.value.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.5.attention.self.value.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.5.attention.output.dense.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.5.attention.output.dense.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.5.intermediate.dense.weight
2023-04-19 18:23:18,263 n: bert.encoder.layer.5.intermediate.dense.bias
2023-04-19 18:23:18,263 n: bert.encoder.layer.5.output.dense.weight
2023-04-19 18:23:18,264 n: bert.encoder.layer.5.output.dense.bias
2023-04-19 18:23:18,264 n: bert.encoder.layer.5.output.LayerNorm.weight
2023-04-19 18:23:18,264 n: bert.encoder.layer.5.output.LayerNorm.bias
2023-04-19 18:23:18,264 n: bert.pooler.dense.weight
2023-04-19 18:23:18,264 n: bert.pooler.dense.bias
2023-04-19 18:23:18,264 n: classifier.weight
2023-04-19 18:23:18,264 n: classifier.bias
2023-04-19 18:23:18,264 n: fit_dense.weight
2023-04-19 18:23:18,264 n: fit_dense.bias
2023-04-19 18:23:18,264 Total parameters: 67547138
2023-04-19 18:34:46,488 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-cased-finetuned-sst2', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=True, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-19 18:34:46,488 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-19 18:34:46,488 device: cpu n_gpu: 0
2023-04-19 18:34:46,505 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-19 18:34:46,550 *** Example ***
2023-04-19 18:34:46,550 guid: dev-1
2023-04-19 18:34:46,550 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-19 18:34:46,550 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:34:46,550 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:34:46,550 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:34:46,550 label: 1
2023-04-19 18:34:46,550 label_id: 1
2023-04-19 18:34:46,804 Model config
2023-04-19 18:34:48,895 Loading model models/GeneralDistilledModels/TinyBERT_General_6L_768D/pytorch_model.bin
2023-04-19 18:34:48,943 loading model...
2023-04-19 18:34:48,954 done!
2023-04-19 18:34:48,955 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-04-19 18:34:48,955 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2023-04-19 18:34:48,956 ***** Running evaluation *****
2023-04-19 18:34:48,956   Num examples = 872
2023-04-19 18:34:48,956   Batch size = 32
2023-04-19 18:35:06,437 ***** Eval results *****
2023-04-19 18:35:06,437   acc = 0.5871559633027523
2023-04-19 18:35:06,437   eval_loss = 0.6920072159596852
2023-04-19 18:36:26,243 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-cased-finetuned-sst2', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=True, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-19 18:36:26,244 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-19 18:36:26,244 device: cpu n_gpu: 0
2023-04-19 18:36:26,259 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-19 18:36:26,269 *** Example ***
2023-04-19 18:36:26,269 guid: dev-1
2023-04-19 18:36:26,269 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-19 18:36:26,269 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:36:26,269 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:36:26,269 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:36:26,269 label: 1
2023-04-19 18:36:26,269 label_id: 1
2023-04-19 18:36:26,423 Model config
2023-04-19 18:36:28,468 Loading model models/GeneralDistilledModels/TinyBERT_General_6L_768D/pytorch_model.bin
2023-04-19 18:36:28,515 loading model...
2023-04-19 18:36:28,525 done!
2023-04-19 18:36:28,525 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-04-19 18:36:28,525 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2023-04-19 18:36:28,526 ***** Running evaluation *****
2023-04-19 18:36:28,526   Num examples = 872
2023-04-19 18:36:28,526   Batch size = 32
2023-04-19 18:36:45,649 ***** Eval results *****
2023-04-19 18:36:45,649   acc = 0.5871559633027523
2023-04-19 18:36:45,649   eval_loss = 0.6920072159596852
2023-04-19 18:37:39,650 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-cased-finetuned-sst2', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-19 18:37:39,650 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-19 18:37:39,650 device: cpu n_gpu: 0
2023-04-19 18:37:42,292 *** Example ***
2023-04-19 18:37:42,293 guid: aug-1
2023-04-19 18:37:42,293 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-04-19 18:37:42,293 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:37:42,293 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:37:42,293 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:37:42,293 label: 0
2023-04-19 18:37:42,293 label_id: 0
2023-04-19 18:37:45,319 Converted and tokenised the training data.
2023-04-19 18:37:45,319 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-19 18:37:45,324 *** Example ***
2023-04-19 18:37:45,324 guid: dev-1
2023-04-19 18:37:45,324 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-19 18:37:45,324 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:37:45,324 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:37:45,324 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:37:45,324 label: 1
2023-04-19 18:37:45,324 label_id: 1
2023-04-19 18:37:45,469 Model config
2023-04-19 18:37:48,690 Loading model models/FineTunedModels/bert-base-cased-finetuned-sst2/pytorch_model.bin
2023-04-19 18:37:48,781 loading model...
2023-04-19 18:37:48,812 done!
2023-04-19 18:37:48,812 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-04-19 18:37:48,812 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2023-04-19 18:37:48,814 Model config
2023-04-19 18:37:50,947 Loading model models/GeneralDistilledModels/TinyBERT_General_6L_768D/pytorch_model.bin
2023-04-19 18:37:51,078 loading model...
2023-04-19 18:37:51,092 done!
2023-04-19 18:37:51,092 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-04-19 18:37:51,092 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2023-04-19 18:37:51,093 ***** Running training *****
2023-04-19 18:37:51,093   Num examples = 1119160
2023-04-19 18:37:51,093   Batch size = 32
2023-04-19 18:37:51,093   Num steps = 34973
2023-04-19 18:37:51,094 n: bert.embeddings.word_embeddings.weight
2023-04-19 18:37:51,094 n: bert.embeddings.position_embeddings.weight
2023-04-19 18:37:51,094 n: bert.embeddings.token_type_embeddings.weight
2023-04-19 18:37:51,094 n: bert.embeddings.LayerNorm.weight
2023-04-19 18:37:51,094 n: bert.embeddings.LayerNorm.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.attention.self.query.weight
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.attention.self.query.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.attention.self.key.weight
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.attention.self.key.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.attention.self.value.weight
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.attention.self.value.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.attention.output.dense.weight
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.attention.output.dense.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.intermediate.dense.weight
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.intermediate.dense.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.output.dense.weight
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.output.dense.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-04-19 18:37:51,094 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.1.attention.self.query.weight
2023-04-19 18:37:51,094 n: bert.encoder.layer.1.attention.self.query.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.1.attention.self.key.weight
2023-04-19 18:37:51,094 n: bert.encoder.layer.1.attention.self.key.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.1.attention.self.value.weight
2023-04-19 18:37:51,094 n: bert.encoder.layer.1.attention.self.value.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.1.attention.output.dense.weight
2023-04-19 18:37:51,094 n: bert.encoder.layer.1.attention.output.dense.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-04-19 18:37:51,094 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-04-19 18:37:51,094 n: bert.encoder.layer.1.intermediate.dense.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.1.intermediate.dense.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.1.output.dense.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.1.output.dense.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.attention.self.query.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.attention.self.query.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.attention.self.key.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.attention.self.key.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.attention.self.value.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.attention.self.value.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.attention.output.dense.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.attention.output.dense.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.intermediate.dense.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.intermediate.dense.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.output.dense.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.output.dense.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.attention.self.query.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.attention.self.query.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.attention.self.key.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.attention.self.key.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.attention.self.value.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.attention.self.value.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.attention.output.dense.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.attention.output.dense.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.intermediate.dense.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.intermediate.dense.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.output.dense.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.output.dense.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.4.attention.self.query.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.4.attention.self.query.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.4.attention.self.key.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.4.attention.self.key.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.4.attention.self.value.weight
2023-04-19 18:37:51,095 n: bert.encoder.layer.4.attention.self.value.bias
2023-04-19 18:37:51,095 n: bert.encoder.layer.4.attention.output.dense.weight
2023-04-19 18:37:51,096 n: bert.encoder.layer.4.attention.output.dense.bias
2023-04-19 18:37:51,096 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2023-04-19 18:37:51,096 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2023-04-19 18:37:51,096 n: bert.encoder.layer.4.intermediate.dense.weight
2023-04-19 18:37:51,096 n: bert.encoder.layer.4.intermediate.dense.bias
2023-04-19 18:37:51,096 n: bert.encoder.layer.4.output.dense.weight
2023-04-19 18:37:51,096 n: bert.encoder.layer.4.output.dense.bias
2023-04-19 18:37:51,096 n: bert.encoder.layer.4.output.LayerNorm.weight
2023-04-19 18:37:51,096 n: bert.encoder.layer.4.output.LayerNorm.bias
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.attention.self.query.weight
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.attention.self.query.bias
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.attention.self.key.weight
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.attention.self.key.bias
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.attention.self.value.weight
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.attention.self.value.bias
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.attention.output.dense.weight
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.attention.output.dense.bias
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.intermediate.dense.weight
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.intermediate.dense.bias
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.output.dense.weight
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.output.dense.bias
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.output.LayerNorm.weight
2023-04-19 18:37:51,096 n: bert.encoder.layer.5.output.LayerNorm.bias
2023-04-19 18:37:51,096 n: bert.pooler.dense.weight
2023-04-19 18:37:51,096 n: bert.pooler.dense.bias
2023-04-19 18:37:51,096 n: classifier.weight
2023-04-19 18:37:51,096 n: classifier.bias
2023-04-19 18:37:51,096 n: fit_dense.weight
2023-04-19 18:37:51,096 n: fit_dense.bias
2023-04-19 18:37:51,096 Total parameters: 67547138
2023-04-19 18:38:50,950 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-cased-finetuned-sst2', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-19 18:38:50,950 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-19 18:38:50,950 device: cpu n_gpu: 0
2023-04-19 18:38:53,558 *** Example ***
2023-04-19 18:38:53,558 guid: aug-1
2023-04-19 18:38:53,558 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-04-19 18:38:53,558 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:38:53,559 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:38:53,559 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:38:53,559 label: 0
2023-04-19 18:38:53,559 label_id: 0
2023-04-19 18:38:56,534 Converted and tokenised the training data.
2023-04-19 18:38:56,535 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-19 18:38:56,539 *** Example ***
2023-04-19 18:38:56,539 guid: dev-1
2023-04-19 18:38:56,539 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-19 18:38:56,539 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:38:56,539 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:38:56,539 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:38:56,539 label: 1
2023-04-19 18:38:56,539 label_id: 1
2023-04-19 18:38:56,681 Model config
2023-04-19 18:38:59,845 Loading model models/FineTunedModels/bert-base-cased-finetuned-sst2/pytorch_model.bin
2023-04-19 18:38:59,917 loading model...
2023-04-19 18:38:59,936 done!
2023-04-19 18:38:59,937 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-04-19 18:38:59,937 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2023-04-19 18:38:59,938 Model config
2023-04-19 18:39:01,990 Loading model models/GeneralDistilledModels/TinyBERT_General_6L_768D/pytorch_model.bin
2023-04-19 18:39:02,037 loading model...
2023-04-19 18:39:02,047 done!
2023-04-19 18:39:02,047 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-04-19 18:39:02,048 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2023-04-19 18:39:02,048 ***** Running training *****
2023-04-19 18:39:02,048   Num examples = 1119160
2023-04-19 18:39:02,048   Batch size = 32
2023-04-19 18:39:02,048   Num steps = 34973
2023-04-19 18:39:02,049 n: bert.embeddings.word_embeddings.weight
2023-04-19 18:39:02,049 n: bert.embeddings.position_embeddings.weight
2023-04-19 18:39:02,049 n: bert.embeddings.token_type_embeddings.weight
2023-04-19 18:39:02,049 n: bert.embeddings.LayerNorm.weight
2023-04-19 18:39:02,049 n: bert.embeddings.LayerNorm.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.attention.self.query.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.attention.self.query.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.attention.self.key.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.attention.self.key.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.attention.self.value.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.attention.self.value.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.attention.output.dense.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.attention.output.dense.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.intermediate.dense.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.intermediate.dense.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.output.dense.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.output.dense.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.attention.self.query.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.attention.self.query.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.attention.self.key.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.attention.self.key.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.attention.self.value.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.attention.self.value.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.attention.output.dense.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.attention.output.dense.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.intermediate.dense.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.intermediate.dense.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.output.dense.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.output.dense.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.2.attention.self.query.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.2.attention.self.query.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.2.attention.self.key.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.2.attention.self.key.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.2.attention.self.value.weight
2023-04-19 18:39:02,049 n: bert.encoder.layer.2.attention.self.value.bias
2023-04-19 18:39:02,049 n: bert.encoder.layer.2.attention.output.dense.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.2.attention.output.dense.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.2.intermediate.dense.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.2.intermediate.dense.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.2.output.dense.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.2.output.dense.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.attention.self.query.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.attention.self.query.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.attention.self.key.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.attention.self.key.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.attention.self.value.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.attention.self.value.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.attention.output.dense.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.attention.output.dense.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.intermediate.dense.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.intermediate.dense.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.output.dense.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.output.dense.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.attention.self.query.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.attention.self.query.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.attention.self.key.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.attention.self.key.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.attention.self.value.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.attention.self.value.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.attention.output.dense.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.attention.output.dense.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.intermediate.dense.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.intermediate.dense.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.output.dense.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.output.dense.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.output.LayerNorm.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.4.output.LayerNorm.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.5.attention.self.query.weight
2023-04-19 18:39:02,050 n: bert.encoder.layer.5.attention.self.query.bias
2023-04-19 18:39:02,050 n: bert.encoder.layer.5.attention.self.key.weight
2023-04-19 18:39:02,051 n: bert.encoder.layer.5.attention.self.key.bias
2023-04-19 18:39:02,051 n: bert.encoder.layer.5.attention.self.value.weight
2023-04-19 18:39:02,051 n: bert.encoder.layer.5.attention.self.value.bias
2023-04-19 18:39:02,051 n: bert.encoder.layer.5.attention.output.dense.weight
2023-04-19 18:39:02,051 n: bert.encoder.layer.5.attention.output.dense.bias
2023-04-19 18:39:02,051 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2023-04-19 18:39:02,051 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2023-04-19 18:39:02,051 n: bert.encoder.layer.5.intermediate.dense.weight
2023-04-19 18:39:02,051 n: bert.encoder.layer.5.intermediate.dense.bias
2023-04-19 18:39:02,051 n: bert.encoder.layer.5.output.dense.weight
2023-04-19 18:39:02,051 n: bert.encoder.layer.5.output.dense.bias
2023-04-19 18:39:02,051 n: bert.encoder.layer.5.output.LayerNorm.weight
2023-04-19 18:39:02,051 n: bert.encoder.layer.5.output.LayerNorm.bias
2023-04-19 18:39:02,051 n: bert.pooler.dense.weight
2023-04-19 18:39:02,051 n: bert.pooler.dense.bias
2023-04-19 18:39:02,051 n: classifier.weight
2023-04-19 18:39:02,051 n: classifier.bias
2023-04-19 18:39:02,051 n: fit_dense.weight
2023-04-19 18:39:02,051 n: fit_dense.bias
2023-04-19 18:39:02,051 Total parameters: 67547138
2023-04-19 18:39:09,232 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-cased-finetuned-sst2', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-19 18:39:09,232 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-19 18:39:09,232 device: cpu n_gpu: 0
2023-04-19 18:39:18,870 *** Example ***
2023-04-19 18:39:18,871 guid: aug-1
2023-04-19 18:39:18,871 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-04-19 18:39:18,871 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:39:18,871 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:39:18,872 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:39:18,872 label: 0
2023-04-19 18:39:18,872 label_id: 0
2023-04-19 18:39:28,302 Converted and tokenised the training data.
2023-04-19 18:39:28,302 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-19 18:39:28,306 *** Example ***
2023-04-19 18:39:28,306 guid: dev-1
2023-04-19 18:39:28,306 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-19 18:39:28,306 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:39:28,306 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:39:28,306 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 18:39:28,306 label: 1
2023-04-19 18:39:28,306 label_id: 1
2023-04-19 18:39:42,589 Model config
2023-04-19 18:39:45,768 Loading model models/FineTunedModels/bert-base-cased-finetuned-sst2/pytorch_model.bin
2023-04-19 18:39:45,845 loading model...
2023-04-19 18:39:45,876 done!
2023-04-19 18:39:45,876 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-04-19 18:39:45,876 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2023-04-19 18:50:15,795 Model config
2023-04-19 18:50:17,862 Loading model models/GeneralDistilledModels/TinyBERT_General_6L_768D/pytorch_model.bin
2023-04-19 18:50:17,960 loading model...
2023-04-19 18:50:17,983 done!
2023-04-19 18:50:17,983 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-04-19 18:50:17,983 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2023-04-19 18:51:08,988 ***** Running training *****
2023-04-19 18:51:09,791   Num examples = 1119160
2023-04-19 18:51:12,421   Batch size = 32
2023-04-19 18:51:47,725   Num steps = 34973
2023-04-19 18:51:56,843 n: bert.embeddings.word_embeddings.weight
2023-04-19 23:06:49,284 n: bert.embeddings.position_embeddings.weight
2023-04-19 23:06:53,989 n: bert.embeddings.token_type_embeddings.weight
2023-04-19 23:06:53,990 n: bert.embeddings.LayerNorm.weight
2023-04-19 23:06:53,990 n: bert.embeddings.LayerNorm.bias
2023-04-19 23:06:53,990 n: bert.encoder.layer.0.attention.self.query.weight
2023-04-19 23:06:53,991 n: bert.encoder.layer.0.attention.self.query.bias
2023-04-19 23:06:53,991 n: bert.encoder.layer.0.attention.self.key.weight
2023-04-19 23:06:53,991 n: bert.encoder.layer.0.attention.self.key.bias
2023-04-19 23:06:53,991 n: bert.encoder.layer.0.attention.self.value.weight
2023-04-19 23:06:53,991 n: bert.encoder.layer.0.attention.self.value.bias
2023-04-19 23:06:53,991 n: bert.encoder.layer.0.attention.output.dense.weight
2023-04-19 23:06:53,991 n: bert.encoder.layer.0.attention.output.dense.bias
2023-04-19 23:06:53,992 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-04-19 23:06:53,992 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-04-19 23:06:53,992 n: bert.encoder.layer.0.intermediate.dense.weight
2023-04-19 23:06:53,992 n: bert.encoder.layer.0.intermediate.dense.bias
2023-04-19 23:06:53,992 n: bert.encoder.layer.0.output.dense.weight
2023-04-19 23:06:53,992 n: bert.encoder.layer.0.output.dense.bias
2023-04-19 23:06:53,992 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-04-19 23:06:53,992 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-04-19 23:06:53,993 n: bert.encoder.layer.1.attention.self.query.weight
2023-04-19 23:06:53,993 n: bert.encoder.layer.1.attention.self.query.bias
2023-04-19 23:06:53,993 n: bert.encoder.layer.1.attention.self.key.weight
2023-04-19 23:06:53,993 n: bert.encoder.layer.1.attention.self.key.bias
2023-04-19 23:06:53,993 n: bert.encoder.layer.1.attention.self.value.weight
2023-04-19 23:06:53,993 n: bert.encoder.layer.1.attention.self.value.bias
2023-04-19 23:06:53,993 n: bert.encoder.layer.1.attention.output.dense.weight
2023-04-19 23:06:53,994 n: bert.encoder.layer.1.attention.output.dense.bias
2023-04-19 23:06:53,994 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-04-19 23:06:53,994 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-04-19 23:06:53,994 n: bert.encoder.layer.1.intermediate.dense.weight
2023-04-19 23:06:53,994 n: bert.encoder.layer.1.intermediate.dense.bias
2023-04-19 23:06:53,995 n: bert.encoder.layer.1.output.dense.weight
2023-04-19 23:06:53,995 n: bert.encoder.layer.1.output.dense.bias
2023-04-19 23:06:53,995 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-04-19 23:06:53,995 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-04-19 23:06:53,995 n: bert.encoder.layer.2.attention.self.query.weight
2023-04-19 23:06:53,995 n: bert.encoder.layer.2.attention.self.query.bias
2023-04-19 23:06:53,995 n: bert.encoder.layer.2.attention.self.key.weight
2023-04-19 23:06:53,995 n: bert.encoder.layer.2.attention.self.key.bias
2023-04-19 23:06:53,996 n: bert.encoder.layer.2.attention.self.value.weight
2023-04-19 23:06:53,996 n: bert.encoder.layer.2.attention.self.value.bias
2023-04-19 23:06:53,996 n: bert.encoder.layer.2.attention.output.dense.weight
2023-04-19 23:06:53,996 n: bert.encoder.layer.2.attention.output.dense.bias
2023-04-19 23:06:53,996 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-04-19 23:06:53,996 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-04-19 23:06:53,996 n: bert.encoder.layer.2.intermediate.dense.weight
2023-04-19 23:06:53,996 n: bert.encoder.layer.2.intermediate.dense.bias
2023-04-19 23:06:53,996 n: bert.encoder.layer.2.output.dense.weight
2023-04-19 23:06:53,997 n: bert.encoder.layer.2.output.dense.bias
2023-04-19 23:06:53,997 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-04-19 23:06:53,997 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-04-19 23:06:53,997 n: bert.encoder.layer.3.attention.self.query.weight
2023-04-19 23:06:53,997 n: bert.encoder.layer.3.attention.self.query.bias
2023-04-19 23:06:53,997 n: bert.encoder.layer.3.attention.self.key.weight
2023-04-19 23:06:53,997 n: bert.encoder.layer.3.attention.self.key.bias
2023-04-19 23:06:53,997 n: bert.encoder.layer.3.attention.self.value.weight
2023-04-19 23:06:53,998 n: bert.encoder.layer.3.attention.self.value.bias
2023-04-19 23:06:53,998 n: bert.encoder.layer.3.attention.output.dense.weight
2023-04-19 23:06:53,998 n: bert.encoder.layer.3.attention.output.dense.bias
2023-04-19 23:06:53,998 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-04-19 23:06:53,998 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-04-19 23:06:53,998 n: bert.encoder.layer.3.intermediate.dense.weight
2023-04-19 23:06:53,998 n: bert.encoder.layer.3.intermediate.dense.bias
2023-04-19 23:06:53,998 n: bert.encoder.layer.3.output.dense.weight
2023-04-19 23:06:53,999 n: bert.encoder.layer.3.output.dense.bias
2023-04-19 23:06:53,999 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-04-19 23:06:53,999 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-04-19 23:06:53,999 n: bert.encoder.layer.4.attention.self.query.weight
2023-04-19 23:06:53,999 n: bert.encoder.layer.4.attention.self.query.bias
2023-04-19 23:06:53,999 n: bert.encoder.layer.4.attention.self.key.weight
2023-04-19 23:06:53,999 n: bert.encoder.layer.4.attention.self.key.bias
2023-04-19 23:06:53,999 n: bert.encoder.layer.4.attention.self.value.weight
2023-04-19 23:06:53,999 n: bert.encoder.layer.4.attention.self.value.bias
2023-04-19 23:06:54,000 n: bert.encoder.layer.4.attention.output.dense.weight
2023-04-19 23:06:54,000 n: bert.encoder.layer.4.attention.output.dense.bias
2023-04-19 23:06:54,000 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2023-04-19 23:06:54,000 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2023-04-19 23:06:54,000 n: bert.encoder.layer.4.intermediate.dense.weight
2023-04-19 23:06:54,000 n: bert.encoder.layer.4.intermediate.dense.bias
2023-04-19 23:06:54,001 n: bert.encoder.layer.4.output.dense.weight
2023-04-19 23:06:54,001 n: bert.encoder.layer.4.output.dense.bias
2023-04-19 23:06:54,001 n: bert.encoder.layer.4.output.LayerNorm.weight
2023-04-19 23:06:54,001 n: bert.encoder.layer.4.output.LayerNorm.bias
2023-04-19 23:06:54,001 n: bert.encoder.layer.5.attention.self.query.weight
2023-04-19 23:06:54,001 n: bert.encoder.layer.5.attention.self.query.bias
2023-04-19 23:06:54,001 n: bert.encoder.layer.5.attention.self.key.weight
2023-04-19 23:06:54,001 n: bert.encoder.layer.5.attention.self.key.bias
2023-04-19 23:06:54,002 n: bert.encoder.layer.5.attention.self.value.weight
2023-04-19 23:06:54,002 n: bert.encoder.layer.5.attention.self.value.bias
2023-04-19 23:06:54,002 n: bert.encoder.layer.5.attention.output.dense.weight
2023-04-19 23:06:54,002 n: bert.encoder.layer.5.attention.output.dense.bias
2023-04-19 23:06:54,002 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2023-04-19 23:06:54,002 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2023-04-19 23:06:54,002 n: bert.encoder.layer.5.intermediate.dense.weight
2023-04-19 23:06:54,003 n: bert.encoder.layer.5.intermediate.dense.bias
2023-04-19 23:06:54,003 n: bert.encoder.layer.5.output.dense.weight
2023-04-19 23:06:54,003 n: bert.encoder.layer.5.output.dense.bias
2023-04-19 23:06:54,003 n: bert.encoder.layer.5.output.LayerNorm.weight
2023-04-19 23:06:54,003 n: bert.encoder.layer.5.output.LayerNorm.bias
2023-04-19 23:06:54,003 n: bert.pooler.dense.weight
2023-04-19 23:06:54,003 n: bert.pooler.dense.bias
2023-04-19 23:06:54,003 n: classifier.weight
2023-04-19 23:06:54,003 n: classifier.bias
2023-04-19 23:06:54,004 n: fit_dense.weight
2023-04-19 23:06:54,004 n: fit_dense.bias
2023-04-19 23:06:54,004 Total parameters: 67547138
2023-04-19 23:16:34,517 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-cased-finetuned-sst2', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-19 23:16:34,517 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-19 23:16:34,518 device: cpu n_gpu: 0
2023-04-19 23:16:37,950 *** Example ***
2023-04-19 23:16:37,950 guid: aug-1
2023-04-19 23:16:37,950 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-04-19 23:16:37,950 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:16:37,951 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:16:37,951 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:16:37,951 label: 0
2023-04-19 23:16:37,951 label_id: 0
2023-04-19 23:16:47,567 Converted and tokenised the training data.
2023-04-19 23:16:47,567 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-19 23:16:47,571 *** Example ***
2023-04-19 23:16:47,571 guid: dev-1
2023-04-19 23:16:47,571 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-19 23:16:47,572 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:16:47,572 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:16:47,572 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:16:47,572 label: 1
2023-04-19 23:16:47,572 label_id: 1
2023-04-19 23:26:58,711 Model config
2023-04-19 23:27:01,908 Loading model models/FineTunedModels/bert-base-cased-finetuned-sst2/pytorch_model.bin
2023-04-19 23:27:02,065 loading model...
2023-04-19 23:27:02,108 done!
2023-04-19 23:27:02,108 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-04-19 23:27:02,108 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2023-04-19 23:27:02,111 Model config
2023-04-19 23:27:04,190 Loading model models/GeneralDistilledModels/TinyBERT_General_6L_768D/pytorch_model.bin
2023-04-19 23:27:04,375 loading model...
2023-04-19 23:27:04,395 done!
2023-04-19 23:27:04,395 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-04-19 23:27:04,395 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2023-04-19 23:27:04,397 ***** Running training *****
2023-04-19 23:27:04,397   Num examples = 1119160
2023-04-19 23:27:04,397   Batch size = 32
2023-04-19 23:27:04,397   Num steps = 34973
2023-04-19 23:27:04,398 n: bert.embeddings.word_embeddings.weight
2023-04-19 23:27:04,398 n: bert.embeddings.position_embeddings.weight
2023-04-19 23:27:04,398 n: bert.embeddings.token_type_embeddings.weight
2023-04-19 23:27:04,399 n: bert.embeddings.LayerNorm.weight
2023-04-19 23:27:04,399 n: bert.embeddings.LayerNorm.bias
2023-04-19 23:27:04,399 n: bert.encoder.layer.0.attention.self.query.weight
2023-04-19 23:27:04,399 n: bert.encoder.layer.0.attention.self.query.bias
2023-04-19 23:27:04,399 n: bert.encoder.layer.0.attention.self.key.weight
2023-04-19 23:27:04,399 n: bert.encoder.layer.0.attention.self.key.bias
2023-04-19 23:27:04,399 n: bert.encoder.layer.0.attention.self.value.weight
2023-04-19 23:27:04,399 n: bert.encoder.layer.0.attention.self.value.bias
2023-04-19 23:27:04,399 n: bert.encoder.layer.0.attention.output.dense.weight
2023-04-19 23:27:04,399 n: bert.encoder.layer.0.attention.output.dense.bias
2023-04-19 23:27:04,399 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-04-19 23:27:04,399 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-04-19 23:27:04,400 n: bert.encoder.layer.0.intermediate.dense.weight
2023-04-19 23:27:04,400 n: bert.encoder.layer.0.intermediate.dense.bias
2023-04-19 23:27:04,400 n: bert.encoder.layer.0.output.dense.weight
2023-04-19 23:27:04,400 n: bert.encoder.layer.0.output.dense.bias
2023-04-19 23:27:04,400 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-04-19 23:27:04,400 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-04-19 23:27:04,400 n: bert.encoder.layer.1.attention.self.query.weight
2023-04-19 23:27:04,400 n: bert.encoder.layer.1.attention.self.query.bias
2023-04-19 23:27:04,400 n: bert.encoder.layer.1.attention.self.key.weight
2023-04-19 23:27:04,400 n: bert.encoder.layer.1.attention.self.key.bias
2023-04-19 23:27:04,400 n: bert.encoder.layer.1.attention.self.value.weight
2023-04-19 23:27:04,400 n: bert.encoder.layer.1.attention.self.value.bias
2023-04-19 23:27:04,401 n: bert.encoder.layer.1.attention.output.dense.weight
2023-04-19 23:27:04,401 n: bert.encoder.layer.1.attention.output.dense.bias
2023-04-19 23:27:04,401 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-04-19 23:27:04,401 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-04-19 23:27:04,401 n: bert.encoder.layer.1.intermediate.dense.weight
2023-04-19 23:27:04,401 n: bert.encoder.layer.1.intermediate.dense.bias
2023-04-19 23:27:04,401 n: bert.encoder.layer.1.output.dense.weight
2023-04-19 23:27:04,401 n: bert.encoder.layer.1.output.dense.bias
2023-04-19 23:27:04,401 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-04-19 23:27:04,401 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-04-19 23:27:04,401 n: bert.encoder.layer.2.attention.self.query.weight
2023-04-19 23:27:04,401 n: bert.encoder.layer.2.attention.self.query.bias
2023-04-19 23:27:04,401 n: bert.encoder.layer.2.attention.self.key.weight
2023-04-19 23:27:04,402 n: bert.encoder.layer.2.attention.self.key.bias
2023-04-19 23:27:04,402 n: bert.encoder.layer.2.attention.self.value.weight
2023-04-19 23:27:04,402 n: bert.encoder.layer.2.attention.self.value.bias
2023-04-19 23:27:04,402 n: bert.encoder.layer.2.attention.output.dense.weight
2023-04-19 23:27:04,402 n: bert.encoder.layer.2.attention.output.dense.bias
2023-04-19 23:27:04,402 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-04-19 23:27:04,402 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-04-19 23:27:04,402 n: bert.encoder.layer.2.intermediate.dense.weight
2023-04-19 23:27:04,402 n: bert.encoder.layer.2.intermediate.dense.bias
2023-04-19 23:27:04,402 n: bert.encoder.layer.2.output.dense.weight
2023-04-19 23:27:04,402 n: bert.encoder.layer.2.output.dense.bias
2023-04-19 23:27:04,402 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-04-19 23:27:04,402 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-04-19 23:27:04,403 n: bert.encoder.layer.3.attention.self.query.weight
2023-04-19 23:27:04,403 n: bert.encoder.layer.3.attention.self.query.bias
2023-04-19 23:27:04,403 n: bert.encoder.layer.3.attention.self.key.weight
2023-04-19 23:27:04,403 n: bert.encoder.layer.3.attention.self.key.bias
2023-04-19 23:27:04,403 n: bert.encoder.layer.3.attention.self.value.weight
2023-04-19 23:27:04,403 n: bert.encoder.layer.3.attention.self.value.bias
2023-04-19 23:27:04,403 n: bert.encoder.layer.3.attention.output.dense.weight
2023-04-19 23:27:04,403 n: bert.encoder.layer.3.attention.output.dense.bias
2023-04-19 23:27:04,403 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-04-19 23:27:04,403 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-04-19 23:27:04,403 n: bert.encoder.layer.3.intermediate.dense.weight
2023-04-19 23:27:04,403 n: bert.encoder.layer.3.intermediate.dense.bias
2023-04-19 23:27:04,404 n: bert.encoder.layer.3.output.dense.weight
2023-04-19 23:27:04,404 n: bert.encoder.layer.3.output.dense.bias
2023-04-19 23:27:04,404 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-04-19 23:27:04,404 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-04-19 23:27:04,404 n: bert.encoder.layer.4.attention.self.query.weight
2023-04-19 23:27:04,404 n: bert.encoder.layer.4.attention.self.query.bias
2023-04-19 23:27:04,404 n: bert.encoder.layer.4.attention.self.key.weight
2023-04-19 23:27:04,404 n: bert.encoder.layer.4.attention.self.key.bias
2023-04-19 23:27:04,404 n: bert.encoder.layer.4.attention.self.value.weight
2023-04-19 23:27:04,404 n: bert.encoder.layer.4.attention.self.value.bias
2023-04-19 23:27:04,404 n: bert.encoder.layer.4.attention.output.dense.weight
2023-04-19 23:27:04,404 n: bert.encoder.layer.4.attention.output.dense.bias
2023-04-19 23:27:04,404 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2023-04-19 23:27:04,405 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2023-04-19 23:27:04,405 n: bert.encoder.layer.4.intermediate.dense.weight
2023-04-19 23:27:04,405 n: bert.encoder.layer.4.intermediate.dense.bias
2023-04-19 23:27:04,405 n: bert.encoder.layer.4.output.dense.weight
2023-04-19 23:27:04,405 n: bert.encoder.layer.4.output.dense.bias
2023-04-19 23:27:04,405 n: bert.encoder.layer.4.output.LayerNorm.weight
2023-04-19 23:27:04,405 n: bert.encoder.layer.4.output.LayerNorm.bias
2023-04-19 23:27:04,405 n: bert.encoder.layer.5.attention.self.query.weight
2023-04-19 23:27:04,405 n: bert.encoder.layer.5.attention.self.query.bias
2023-04-19 23:27:04,405 n: bert.encoder.layer.5.attention.self.key.weight
2023-04-19 23:27:04,406 n: bert.encoder.layer.5.attention.self.key.bias
2023-04-19 23:27:04,406 n: bert.encoder.layer.5.attention.self.value.weight
2023-04-19 23:27:04,406 n: bert.encoder.layer.5.attention.self.value.bias
2023-04-19 23:27:04,406 n: bert.encoder.layer.5.attention.output.dense.weight
2023-04-19 23:27:04,406 n: bert.encoder.layer.5.attention.output.dense.bias
2023-04-19 23:27:04,406 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2023-04-19 23:27:04,406 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2023-04-19 23:27:04,406 n: bert.encoder.layer.5.intermediate.dense.weight
2023-04-19 23:27:04,406 n: bert.encoder.layer.5.intermediate.dense.bias
2023-04-19 23:27:04,406 n: bert.encoder.layer.5.output.dense.weight
2023-04-19 23:27:04,406 n: bert.encoder.layer.5.output.dense.bias
2023-04-19 23:27:04,406 n: bert.encoder.layer.5.output.LayerNorm.weight
2023-04-19 23:27:04,406 n: bert.encoder.layer.5.output.LayerNorm.bias
2023-04-19 23:27:04,407 n: bert.pooler.dense.weight
2023-04-19 23:27:04,407 n: bert.pooler.dense.bias
2023-04-19 23:27:04,407 n: classifier.weight
2023-04-19 23:27:04,407 n: classifier.bias
2023-04-19 23:27:04,407 n: fit_dense.weight
2023-04-19 23:27:04,407 n: fit_dense.bias
2023-04-19 23:27:08,418 Total parameters: 67547138
2023-04-19 23:33:13,484 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-cased-finetuned-sst2', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-19 23:33:13,485 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-19 23:33:13,485 device: cpu n_gpu: 0
2023-04-19 23:34:21,346 *** Example ***
2023-04-19 23:34:21,346 guid: aug-1
2023-04-19 23:34:21,347 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-04-19 23:34:21,347 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:34:21,347 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:34:21,347 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:34:21,347 label: 0
2023-04-19 23:34:21,347 label_id: 0
2023-04-19 23:34:31,740 Converted and tokenised the training data.
2023-04-19 23:34:31,740 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-19 23:34:36,021 *** Example ***
2023-04-19 23:34:36,022 guid: dev-1
2023-04-19 23:34:36,022 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-19 23:34:36,022 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:34:36,023 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:34:36,023 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:34:36,023 label: 1
2023-04-19 23:34:36,023 label_id: 1
2023-04-19 23:35:24,540 Model config
2023-04-19 23:35:27,730 Loading model models/FineTunedModels/bert-base-cased-finetuned-sst2/pytorch_model.bin
2023-04-19 23:35:27,828 loading model...
2023-04-19 23:35:27,856 done!
2023-04-19 23:35:27,856 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-04-19 23:35:27,856 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2023-04-19 23:35:46,521 Model config
2023-04-19 23:36:18,442 Loading model models/GeneralDistilledModels/TinyBERT_General_6L_768D/pytorch_model.bin
2023-04-19 23:36:18,501 loading model...
2023-04-19 23:36:18,514 done!
2023-04-19 23:36:18,515 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-04-19 23:36:18,515 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2023-04-19 23:36:18,516 ***** Running training *****
2023-04-19 23:36:18,516   Num examples = 1119160
2023-04-19 23:36:18,516   Batch size = 32
2023-04-19 23:36:18,517   Num steps = 34973
2023-04-19 23:36:18,518 n: bert.embeddings.word_embeddings.weight
2023-04-19 23:36:18,518 n: bert.embeddings.position_embeddings.weight
2023-04-19 23:36:18,518 n: bert.embeddings.token_type_embeddings.weight
2023-04-19 23:36:18,518 n: bert.embeddings.LayerNorm.weight
2023-04-19 23:36:18,518 n: bert.embeddings.LayerNorm.bias
2023-04-19 23:36:18,518 n: bert.encoder.layer.0.attention.self.query.weight
2023-04-19 23:36:18,518 n: bert.encoder.layer.0.attention.self.query.bias
2023-04-19 23:36:18,518 n: bert.encoder.layer.0.attention.self.key.weight
2023-04-19 23:36:18,518 n: bert.encoder.layer.0.attention.self.key.bias
2023-04-19 23:36:18,518 n: bert.encoder.layer.0.attention.self.value.weight
2023-04-19 23:36:18,519 n: bert.encoder.layer.0.attention.self.value.bias
2023-04-19 23:36:18,519 n: bert.encoder.layer.0.attention.output.dense.weight
2023-04-19 23:36:18,519 n: bert.encoder.layer.0.attention.output.dense.bias
2023-04-19 23:36:18,519 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-04-19 23:36:18,519 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-04-19 23:36:18,519 n: bert.encoder.layer.0.intermediate.dense.weight
2023-04-19 23:36:18,519 n: bert.encoder.layer.0.intermediate.dense.bias
2023-04-19 23:36:18,519 n: bert.encoder.layer.0.output.dense.weight
2023-04-19 23:36:18,519 n: bert.encoder.layer.0.output.dense.bias
2023-04-19 23:36:18,519 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-04-19 23:36:18,519 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-04-19 23:36:18,519 n: bert.encoder.layer.1.attention.self.query.weight
2023-04-19 23:36:18,519 n: bert.encoder.layer.1.attention.self.query.bias
2023-04-19 23:36:18,520 n: bert.encoder.layer.1.attention.self.key.weight
2023-04-19 23:36:18,520 n: bert.encoder.layer.1.attention.self.key.bias
2023-04-19 23:36:18,520 n: bert.encoder.layer.1.attention.self.value.weight
2023-04-19 23:36:18,520 n: bert.encoder.layer.1.attention.self.value.bias
2023-04-19 23:36:18,520 n: bert.encoder.layer.1.attention.output.dense.weight
2023-04-19 23:36:18,520 n: bert.encoder.layer.1.attention.output.dense.bias
2023-04-19 23:36:18,520 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-04-19 23:36:18,520 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-04-19 23:36:18,520 n: bert.encoder.layer.1.intermediate.dense.weight
2023-04-19 23:36:18,520 n: bert.encoder.layer.1.intermediate.dense.bias
2023-04-19 23:36:18,520 n: bert.encoder.layer.1.output.dense.weight
2023-04-19 23:36:18,520 n: bert.encoder.layer.1.output.dense.bias
2023-04-19 23:36:18,520 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-04-19 23:36:18,521 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-04-19 23:36:18,521 n: bert.encoder.layer.2.attention.self.query.weight
2023-04-19 23:36:18,521 n: bert.encoder.layer.2.attention.self.query.bias
2023-04-19 23:36:18,521 n: bert.encoder.layer.2.attention.self.key.weight
2023-04-19 23:36:18,521 n: bert.encoder.layer.2.attention.self.key.bias
2023-04-19 23:36:18,521 n: bert.encoder.layer.2.attention.self.value.weight
2023-04-19 23:36:18,521 n: bert.encoder.layer.2.attention.self.value.bias
2023-04-19 23:36:18,521 n: bert.encoder.layer.2.attention.output.dense.weight
2023-04-19 23:36:18,521 n: bert.encoder.layer.2.attention.output.dense.bias
2023-04-19 23:36:18,521 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-04-19 23:36:18,521 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-04-19 23:36:18,521 n: bert.encoder.layer.2.intermediate.dense.weight
2023-04-19 23:36:18,521 n: bert.encoder.layer.2.intermediate.dense.bias
2023-04-19 23:36:18,522 n: bert.encoder.layer.2.output.dense.weight
2023-04-19 23:36:18,522 n: bert.encoder.layer.2.output.dense.bias
2023-04-19 23:36:18,522 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-04-19 23:36:18,522 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-04-19 23:36:18,522 n: bert.encoder.layer.3.attention.self.query.weight
2023-04-19 23:36:18,522 n: bert.encoder.layer.3.attention.self.query.bias
2023-04-19 23:36:18,522 n: bert.encoder.layer.3.attention.self.key.weight
2023-04-19 23:36:18,522 n: bert.encoder.layer.3.attention.self.key.bias
2023-04-19 23:36:18,522 n: bert.encoder.layer.3.attention.self.value.weight
2023-04-19 23:36:18,522 n: bert.encoder.layer.3.attention.self.value.bias
2023-04-19 23:36:18,522 n: bert.encoder.layer.3.attention.output.dense.weight
2023-04-19 23:36:18,522 n: bert.encoder.layer.3.attention.output.dense.bias
2023-04-19 23:36:18,522 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-04-19 23:36:18,523 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-04-19 23:36:18,523 n: bert.encoder.layer.3.intermediate.dense.weight
2023-04-19 23:36:18,523 n: bert.encoder.layer.3.intermediate.dense.bias
2023-04-19 23:36:18,523 n: bert.encoder.layer.3.output.dense.weight
2023-04-19 23:36:18,523 n: bert.encoder.layer.3.output.dense.bias
2023-04-19 23:36:18,523 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-04-19 23:36:18,523 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-04-19 23:36:18,523 n: bert.encoder.layer.4.attention.self.query.weight
2023-04-19 23:36:18,523 n: bert.encoder.layer.4.attention.self.query.bias
2023-04-19 23:36:18,523 n: bert.encoder.layer.4.attention.self.key.weight
2023-04-19 23:36:18,523 n: bert.encoder.layer.4.attention.self.key.bias
2023-04-19 23:36:18,523 n: bert.encoder.layer.4.attention.self.value.weight
2023-04-19 23:36:18,523 n: bert.encoder.layer.4.attention.self.value.bias
2023-04-19 23:36:18,524 n: bert.encoder.layer.4.attention.output.dense.weight
2023-04-19 23:36:18,524 n: bert.encoder.layer.4.attention.output.dense.bias
2023-04-19 23:36:18,524 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2023-04-19 23:36:18,524 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2023-04-19 23:36:18,524 n: bert.encoder.layer.4.intermediate.dense.weight
2023-04-19 23:36:18,524 n: bert.encoder.layer.4.intermediate.dense.bias
2023-04-19 23:36:18,524 n: bert.encoder.layer.4.output.dense.weight
2023-04-19 23:36:18,524 n: bert.encoder.layer.4.output.dense.bias
2023-04-19 23:36:18,524 n: bert.encoder.layer.4.output.LayerNorm.weight
2023-04-19 23:36:18,524 n: bert.encoder.layer.4.output.LayerNorm.bias
2023-04-19 23:36:18,524 n: bert.encoder.layer.5.attention.self.query.weight
2023-04-19 23:36:18,524 n: bert.encoder.layer.5.attention.self.query.bias
2023-04-19 23:36:18,524 n: bert.encoder.layer.5.attention.self.key.weight
2023-04-19 23:36:18,524 n: bert.encoder.layer.5.attention.self.key.bias
2023-04-19 23:36:18,525 n: bert.encoder.layer.5.attention.self.value.weight
2023-04-19 23:36:18,525 n: bert.encoder.layer.5.attention.self.value.bias
2023-04-19 23:36:18,525 n: bert.encoder.layer.5.attention.output.dense.weight
2023-04-19 23:36:18,525 n: bert.encoder.layer.5.attention.output.dense.bias
2023-04-19 23:36:18,525 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2023-04-19 23:36:18,525 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2023-04-19 23:36:18,525 n: bert.encoder.layer.5.intermediate.dense.weight
2023-04-19 23:36:18,525 n: bert.encoder.layer.5.intermediate.dense.bias
2023-04-19 23:36:18,525 n: bert.encoder.layer.5.output.dense.weight
2023-04-19 23:36:18,525 n: bert.encoder.layer.5.output.dense.bias
2023-04-19 23:36:18,525 n: bert.encoder.layer.5.output.LayerNorm.weight
2023-04-19 23:36:18,525 n: bert.encoder.layer.5.output.LayerNorm.bias
2023-04-19 23:36:18,525 n: bert.pooler.dense.weight
2023-04-19 23:36:18,526 n: bert.pooler.dense.bias
2023-04-19 23:36:18,526 n: classifier.weight
2023-04-19 23:36:18,526 n: classifier.bias
2023-04-19 23:36:18,526 n: fit_dense.weight
2023-04-19 23:36:18,526 n: fit_dense.bias
2023-04-19 23:36:18,526 Total parameters: 67547138
2023-04-19 23:37:19,495 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-cased-finetuned-sst2', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-19 23:37:19,496 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-19 23:37:19,496 device: cpu n_gpu: 0
2023-04-19 23:37:31,894 *** Example ***
2023-04-19 23:37:31,894 guid: aug-1
2023-04-19 23:37:31,895 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-04-19 23:37:31,895 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:37:31,895 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:37:31,895 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:37:31,895 label: 0
2023-04-19 23:37:31,895 label_id: 0
2023-04-19 23:37:41,287 Converted and tokenised the training data.
2023-04-19 23:37:41,288 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-19 23:37:41,293 *** Example ***
2023-04-19 23:37:41,293 guid: dev-1
2023-04-19 23:37:41,293 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-19 23:37:41,293 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:37:41,293 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:37:41,293 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:37:41,293 label: 1
2023-04-19 23:37:41,293 label_id: 1
2023-04-19 23:45:40,295 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-cased-finetuned-sst2', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-19 23:45:40,295 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-19 23:45:40,296 device: cpu n_gpu: 0
2023-04-19 23:45:54,896 *** Example ***
2023-04-19 23:45:54,897 guid: aug-1
2023-04-19 23:45:54,897 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-04-19 23:45:54,897 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:45:54,897 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:45:54,897 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:45:54,898 label: 0
2023-04-19 23:45:54,898 label_id: 0
2023-04-19 23:46:02,888 Converted and tokenised the training data.
2023-04-19 23:46:02,888 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-19 23:46:02,891 *** Example ***
2023-04-19 23:46:02,892 guid: dev-1
2023-04-19 23:46:02,892 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-19 23:46:02,892 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:46:02,892 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:46:02,892 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-19 23:46:02,892 label: 1
2023-04-19 23:46:02,892 label_id: 1
2023-04-19 23:46:54,078 Model config
2023-04-19 23:46:59,855 Loading model models/FineTunedModels/bert-base-cased-finetuned-sst2/pytorch_model.bin
2023-04-19 23:46:59,962 loading model...
2023-04-19 23:46:59,994 done!
2023-04-19 23:46:59,994 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-04-19 23:46:59,994 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2023-04-20 10:38:41,727 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-cased-finetuned-sst2', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-20 10:38:41,728 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-20 10:38:41,728 device: cpu n_gpu: 0
2023-04-20 10:38:45,119 *** Example ***
2023-04-20 10:38:45,119 guid: aug-1
2023-04-20 10:38:45,119 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-04-20 10:38:45,119 input_ids: 101 4750 1207 3318 5266 1121 1103 22467 2338 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:38:45,120 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:38:45,120 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:38:45,120 label: 0
2023-04-20 10:38:45,120 label_id: 0
2023-04-20 10:38:54,468 Converted and tokenised the training data.
2023-04-20 10:38:54,468 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-20 10:38:54,473 *** Example ***
2023-04-20 10:38:54,473 guid: dev-1
2023-04-20 10:38:54,473 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-20 10:38:54,473 input_ids: 101 1122 112 188 170 14186 1105 1510 12759 5012 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:38:54,473 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:38:54,473 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:38:54,473 label: 1
2023-04-20 10:38:54,473 label_id: 1
2023-04-20 10:39:59,520 Model config
2023-04-20 10:40:02,734 Loading model models/FineTunedModels/bert-base-cased-finetuned-sst2/pytorch_model.bin
2023-04-20 10:40:02,930 loading model...
2023-04-20 10:40:02,969 done!
2023-04-20 10:40:02,969 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
2023-04-20 10:40:02,969 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2023-04-20 10:40:02,972 Model config
2023-04-20 10:40:05,025 Loading model models/GeneralDistilledModels/TinyBERT_General_6L_768D/pytorch_model.bin
2023-04-20 10:40:05,103 loading model...
2023-04-20 10:40:05,115 done!
2023-04-20 10:40:05,116 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-04-20 10:40:05,116 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2023-04-20 10:52:37,352 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-cased-finetuned-sst2', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-20 10:52:37,352 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-20 10:52:37,353 device: cpu n_gpu: 0
2023-04-20 10:52:40,037 *** Example ***
2023-04-20 10:52:40,037 guid: aug-1
2023-04-20 10:52:40,038 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-04-20 10:52:40,038 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:52:40,038 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:52:40,038 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:52:40,038 label: 0
2023-04-20 10:52:40,038 label_id: 0
2023-04-20 10:52:43,081 Converted and tokenised the training data.
2023-04-20 10:52:43,082 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-20 10:52:43,087 *** Example ***
2023-04-20 10:52:43,087 guid: dev-1
2023-04-20 10:52:43,087 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-20 10:52:43,087 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:52:43,087 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:52:43,087 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:52:43,087 label: 1
2023-04-20 10:52:43,087 label_id: 1
2023-04-20 10:53:45,459 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/OpenVINO/bert-base-uncased-sst2-int8-unstructured80', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-20 10:53:45,459 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-20 10:53:45,459 device: cpu n_gpu: 0
2023-04-20 10:53:48,050 *** Example ***
2023-04-20 10:53:48,051 guid: aug-1
2023-04-20 10:53:48,051 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-04-20 10:53:48,051 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:53:48,051 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:53:48,051 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:53:48,051 label: 0
2023-04-20 10:53:48,051 label_id: 0
2023-04-20 10:53:50,981 Converted and tokenised the training data.
2023-04-20 10:53:50,982 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-20 10:53:50,988 *** Example ***
2023-04-20 10:53:50,988 guid: dev-1
2023-04-20 10:53:50,988 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-20 10:53:50,988 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:53:50,988 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:53:50,988 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:53:50,988 label: 1
2023-04-20 10:53:50,988 label_id: 1
2023-04-20 10:54:46,462 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/OpenVINO/bert-base-uncased-sst2-int8-unstructured80', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-20 10:54:46,462 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-20 10:54:46,462 device: cpu n_gpu: 0
2023-04-20 10:54:49,871 *** Example ***
2023-04-20 10:54:49,872 guid: aug-1
2023-04-20 10:54:49,872 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-04-20 10:54:49,872 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:54:49,872 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:54:49,872 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:54:49,873 label: 0
2023-04-20 10:54:49,873 label_id: 0
2023-04-20 10:54:59,167 Converted and tokenised the training data.
2023-04-20 10:54:59,167 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-20 10:54:59,171 *** Example ***
2023-04-20 10:54:59,171 guid: dev-1
2023-04-20 10:54:59,171 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-20 10:54:59,171 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:54:59,171 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:54:59,171 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:54:59,172 label: 1
2023-04-20 10:54:59,172 label_id: 1
2023-04-20 10:55:29,936 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-uncased-sst2-int8-unstructured80', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-20 10:55:29,937 The default parameters: {'cola': {'num_train_epochs': 50, 'max_seq_length': 64}, 'mnli': {'num_train_epochs': 5, 'max_seq_length': 128}, 'mrpc': {'num_train_epochs': 20, 'max_seq_length': 128}, 'sst-2': {'num_train_epochs': 10, 'max_seq_length': 64}, 'sts-b': {'num_train_epochs': 20, 'max_seq_length': 128}, 'qqp': {'num_train_epochs': 5, 'max_seq_length': 128}, 'qnli': {'num_train_epochs': 10, 'max_seq_length': 128}, 'rte': {'num_train_epochs': 20, 'max_seq_length': 128}}
2023-04-20 10:55:29,937 device: cpu n_gpu: 0
2023-04-20 10:55:33,324 *** Example ***
2023-04-20 10:55:33,324 guid: aug-1
2023-04-20 10:55:33,324 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-04-20 10:55:33,324 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:55:33,324 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:55:33,325 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:55:33,325 label: 0
2023-04-20 10:55:33,325 label_id: 0
2023-04-20 10:55:42,599 Converted and tokenised the training data.
2023-04-20 10:55:42,600 Training Parameters batch--> 32 epoch--> 1.0 grad_acc--> 1
2023-04-20 10:55:42,603 *** Example ***
2023-04-20 10:55:42,603 guid: dev-1
2023-04-20 10:55:42,603 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2023-04-20 10:55:42,604 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:55:42,604 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:55:42,604 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-04-20 10:55:42,604 label: 1
2023-04-20 10:55:42,604 label_id: 1
2023-04-20 10:55:50,604 Model config
2023-04-20 10:55:53,853 Loading model models/FineTunedModels/bert-base-uncased-sst2-int8-unstructured80/pytorch_model.bin
2023-04-20 10:55:54,201 loading model...
2023-04-20 10:55:54,263 done!
2023-04-20 10:55:54,263 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-04-20 10:55:54,263 Weights from pretrained model not used in TinyBertForSequenceClassification: ['nncf_module.bert.embeddings.position_ids', 'nncf_module.bert.embeddings.word_embeddings.weight', 'nncf_module.bert.embeddings.position_embeddings.weight', 'nncf_module.bert.embeddings.token_type_embeddings.weight', 'nncf_module.bert.embeddings.LayerNorm.weight', 'nncf_module.bert.embeddings.LayerNorm.bias', 'nncf_module.bert.encoder.layer.0.attention.self.query.weight', 'nncf_module.bert.encoder.layer.0.attention.self.query.bias', 'nncf_module.bert.encoder.layer.0.attention.self.query.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.0.attention.self.query.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.0.attention.self.query.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.0.attention.self.query.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.0.attention.self.query.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.0.attention.self.key.weight', 'nncf_module.bert.encoder.layer.0.attention.self.key.bias', 'nncf_module.bert.encoder.layer.0.attention.self.key.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.0.attention.self.key.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.0.attention.self.key.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.0.attention.self.key.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.0.attention.self.key.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.0.attention.self.value.weight', 'nncf_module.bert.encoder.layer.0.attention.self.value.bias', 'nncf_module.bert.encoder.layer.0.attention.self.value.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.0.attention.self.value.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.0.attention.self.value.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.0.attention.self.value.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.0.attention.self.value.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.0.attention.output.dense.weight', 'nncf_module.bert.encoder.layer.0.attention.output.dense.bias', 'nncf_module.bert.encoder.layer.0.attention.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.0.attention.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.0.attention.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.0.attention.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.0.attention.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.0.intermediate.dense.weight', 'nncf_module.bert.encoder.layer.0.intermediate.dense.bias', 'nncf_module.bert.encoder.layer.0.intermediate.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.0.intermediate.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.0.intermediate.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.0.intermediate.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.0.intermediate.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.0.output.dense.weight', 'nncf_module.bert.encoder.layer.0.output.dense.bias', 'nncf_module.bert.encoder.layer.0.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.0.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.0.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.0.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.0.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.0.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.0.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.1.attention.self.query.weight', 'nncf_module.bert.encoder.layer.1.attention.self.query.bias', 'nncf_module.bert.encoder.layer.1.attention.self.query.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.1.attention.self.query.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.1.attention.self.query.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.1.attention.self.query.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.1.attention.self.query.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.1.attention.self.key.weight', 'nncf_module.bert.encoder.layer.1.attention.self.key.bias', 'nncf_module.bert.encoder.layer.1.attention.self.key.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.1.attention.self.key.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.1.attention.self.key.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.1.attention.self.key.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.1.attention.self.key.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.1.attention.self.value.weight', 'nncf_module.bert.encoder.layer.1.attention.self.value.bias', 'nncf_module.bert.encoder.layer.1.attention.self.value.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.1.attention.self.value.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.1.attention.self.value.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.1.attention.self.value.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.1.attention.self.value.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.1.attention.output.dense.weight', 'nncf_module.bert.encoder.layer.1.attention.output.dense.bias', 'nncf_module.bert.encoder.layer.1.attention.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.1.attention.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.1.attention.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.1.attention.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.1.attention.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.1.intermediate.dense.weight', 'nncf_module.bert.encoder.layer.1.intermediate.dense.bias', 'nncf_module.bert.encoder.layer.1.intermediate.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.1.intermediate.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.1.intermediate.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.1.intermediate.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.1.intermediate.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.1.output.dense.weight', 'nncf_module.bert.encoder.layer.1.output.dense.bias', 'nncf_module.bert.encoder.layer.1.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.1.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.1.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.1.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.1.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.1.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.1.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.2.attention.self.query.weight', 'nncf_module.bert.encoder.layer.2.attention.self.query.bias', 'nncf_module.bert.encoder.layer.2.attention.self.query.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.2.attention.self.query.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.2.attention.self.query.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.2.attention.self.query.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.2.attention.self.query.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.2.attention.self.key.weight', 'nncf_module.bert.encoder.layer.2.attention.self.key.bias', 'nncf_module.bert.encoder.layer.2.attention.self.key.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.2.attention.self.key.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.2.attention.self.key.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.2.attention.self.key.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.2.attention.self.key.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.2.attention.self.value.weight', 'nncf_module.bert.encoder.layer.2.attention.self.value.bias', 'nncf_module.bert.encoder.layer.2.attention.self.value.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.2.attention.self.value.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.2.attention.self.value.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.2.attention.self.value.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.2.attention.self.value.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.2.attention.output.dense.weight', 'nncf_module.bert.encoder.layer.2.attention.output.dense.bias', 'nncf_module.bert.encoder.layer.2.attention.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.2.attention.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.2.attention.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.2.attention.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.2.attention.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.2.intermediate.dense.weight', 'nncf_module.bert.encoder.layer.2.intermediate.dense.bias', 'nncf_module.bert.encoder.layer.2.intermediate.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.2.intermediate.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.2.intermediate.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.2.intermediate.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.2.intermediate.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.2.output.dense.weight', 'nncf_module.bert.encoder.layer.2.output.dense.bias', 'nncf_module.bert.encoder.layer.2.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.2.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.2.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.2.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.2.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.2.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.2.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.3.attention.self.query.weight', 'nncf_module.bert.encoder.layer.3.attention.self.query.bias', 'nncf_module.bert.encoder.layer.3.attention.self.query.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.3.attention.self.query.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.3.attention.self.query.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.3.attention.self.query.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.3.attention.self.query.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.3.attention.self.key.weight', 'nncf_module.bert.encoder.layer.3.attention.self.key.bias', 'nncf_module.bert.encoder.layer.3.attention.self.key.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.3.attention.self.key.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.3.attention.self.key.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.3.attention.self.key.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.3.attention.self.key.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.3.attention.self.value.weight', 'nncf_module.bert.encoder.layer.3.attention.self.value.bias', 'nncf_module.bert.encoder.layer.3.attention.self.value.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.3.attention.self.value.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.3.attention.self.value.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.3.attention.self.value.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.3.attention.self.value.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.3.attention.output.dense.weight', 'nncf_module.bert.encoder.layer.3.attention.output.dense.bias', 'nncf_module.bert.encoder.layer.3.attention.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.3.attention.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.3.attention.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.3.attention.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.3.attention.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.3.intermediate.dense.weight', 'nncf_module.bert.encoder.layer.3.intermediate.dense.bias', 'nncf_module.bert.encoder.layer.3.intermediate.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.3.intermediate.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.3.intermediate.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.3.intermediate.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.3.intermediate.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.3.output.dense.weight', 'nncf_module.bert.encoder.layer.3.output.dense.bias', 'nncf_module.bert.encoder.layer.3.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.3.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.3.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.3.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.3.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.3.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.3.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.4.attention.self.query.weight', 'nncf_module.bert.encoder.layer.4.attention.self.query.bias', 'nncf_module.bert.encoder.layer.4.attention.self.query.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.4.attention.self.query.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.4.attention.self.query.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.4.attention.self.query.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.4.attention.self.query.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.4.attention.self.key.weight', 'nncf_module.bert.encoder.layer.4.attention.self.key.bias', 'nncf_module.bert.encoder.layer.4.attention.self.key.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.4.attention.self.key.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.4.attention.self.key.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.4.attention.self.key.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.4.attention.self.key.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.4.attention.self.value.weight', 'nncf_module.bert.encoder.layer.4.attention.self.value.bias', 'nncf_module.bert.encoder.layer.4.attention.self.value.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.4.attention.self.value.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.4.attention.self.value.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.4.attention.self.value.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.4.attention.self.value.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.4.attention.output.dense.weight', 'nncf_module.bert.encoder.layer.4.attention.output.dense.bias', 'nncf_module.bert.encoder.layer.4.attention.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.4.attention.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.4.attention.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.4.attention.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.4.attention.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.4.intermediate.dense.weight', 'nncf_module.bert.encoder.layer.4.intermediate.dense.bias', 'nncf_module.bert.encoder.layer.4.intermediate.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.4.intermediate.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.4.intermediate.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.4.intermediate.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.4.intermediate.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.4.output.dense.weight', 'nncf_module.bert.encoder.layer.4.output.dense.bias', 'nncf_module.bert.encoder.layer.4.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.4.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.4.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.4.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.4.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.4.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.4.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.5.attention.self.query.weight', 'nncf_module.bert.encoder.layer.5.attention.self.query.bias', 'nncf_module.bert.encoder.layer.5.attention.self.query.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.5.attention.self.query.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.5.attention.self.query.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.5.attention.self.query.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.5.attention.self.query.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.5.attention.self.key.weight', 'nncf_module.bert.encoder.layer.5.attention.self.key.bias', 'nncf_module.bert.encoder.layer.5.attention.self.key.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.5.attention.self.key.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.5.attention.self.key.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.5.attention.self.key.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.5.attention.self.key.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.5.attention.self.value.weight', 'nncf_module.bert.encoder.layer.5.attention.self.value.bias', 'nncf_module.bert.encoder.layer.5.attention.self.value.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.5.attention.self.value.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.5.attention.self.value.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.5.attention.self.value.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.5.attention.self.value.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.5.attention.output.dense.weight', 'nncf_module.bert.encoder.layer.5.attention.output.dense.bias', 'nncf_module.bert.encoder.layer.5.attention.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.5.attention.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.5.attention.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.5.attention.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.5.attention.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.5.intermediate.dense.weight', 'nncf_module.bert.encoder.layer.5.intermediate.dense.bias', 'nncf_module.bert.encoder.layer.5.intermediate.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.5.intermediate.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.5.intermediate.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.5.intermediate.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.5.intermediate.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.5.output.dense.weight', 'nncf_module.bert.encoder.layer.5.output.dense.bias', 'nncf_module.bert.encoder.layer.5.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.5.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.5.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.5.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.5.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.5.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.5.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.6.attention.self.query.weight', 'nncf_module.bert.encoder.layer.6.attention.self.query.bias', 'nncf_module.bert.encoder.layer.6.attention.self.query.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.6.attention.self.query.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.6.attention.self.query.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.6.attention.self.query.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.6.attention.self.query.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.6.attention.self.key.weight', 'nncf_module.bert.encoder.layer.6.attention.self.key.bias', 'nncf_module.bert.encoder.layer.6.attention.self.key.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.6.attention.self.key.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.6.attention.self.key.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.6.attention.self.key.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.6.attention.self.key.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.6.attention.self.value.weight', 'nncf_module.bert.encoder.layer.6.attention.self.value.bias', 'nncf_module.bert.encoder.layer.6.attention.self.value.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.6.attention.self.value.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.6.attention.self.value.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.6.attention.self.value.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.6.attention.self.value.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.6.attention.output.dense.weight', 'nncf_module.bert.encoder.layer.6.attention.output.dense.bias', 'nncf_module.bert.encoder.layer.6.attention.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.6.attention.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.6.attention.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.6.attention.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.6.attention.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.6.intermediate.dense.weight', 'nncf_module.bert.encoder.layer.6.intermediate.dense.bias', 'nncf_module.bert.encoder.layer.6.intermediate.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.6.intermediate.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.6.intermediate.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.6.intermediate.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.6.intermediate.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.6.output.dense.weight', 'nncf_module.bert.encoder.layer.6.output.dense.bias', 'nncf_module.bert.encoder.layer.6.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.6.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.6.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.6.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.6.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.6.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.6.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.7.attention.self.query.weight', 'nncf_module.bert.encoder.layer.7.attention.self.query.bias', 'nncf_module.bert.encoder.layer.7.attention.self.query.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.7.attention.self.query.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.7.attention.self.query.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.7.attention.self.query.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.7.attention.self.query.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.7.attention.self.key.weight', 'nncf_module.bert.encoder.layer.7.attention.self.key.bias', 'nncf_module.bert.encoder.layer.7.attention.self.key.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.7.attention.self.key.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.7.attention.self.key.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.7.attention.self.key.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.7.attention.self.key.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.7.attention.self.value.weight', 'nncf_module.bert.encoder.layer.7.attention.self.value.bias', 'nncf_module.bert.encoder.layer.7.attention.self.value.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.7.attention.self.value.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.7.attention.self.value.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.7.attention.self.value.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.7.attention.self.value.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.7.attention.output.dense.weight', 'nncf_module.bert.encoder.layer.7.attention.output.dense.bias', 'nncf_module.bert.encoder.layer.7.attention.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.7.attention.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.7.attention.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.7.attention.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.7.attention.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.7.intermediate.dense.weight', 'nncf_module.bert.encoder.layer.7.intermediate.dense.bias', 'nncf_module.bert.encoder.layer.7.intermediate.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.7.intermediate.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.7.intermediate.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.7.intermediate.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.7.intermediate.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.7.output.dense.weight', 'nncf_module.bert.encoder.layer.7.output.dense.bias', 'nncf_module.bert.encoder.layer.7.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.7.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.7.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.7.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.7.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.7.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.7.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.8.attention.self.query.weight', 'nncf_module.bert.encoder.layer.8.attention.self.query.bias', 'nncf_module.bert.encoder.layer.8.attention.self.query.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.8.attention.self.query.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.8.attention.self.query.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.8.attention.self.query.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.8.attention.self.query.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.8.attention.self.key.weight', 'nncf_module.bert.encoder.layer.8.attention.self.key.bias', 'nncf_module.bert.encoder.layer.8.attention.self.key.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.8.attention.self.key.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.8.attention.self.key.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.8.attention.self.key.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.8.attention.self.key.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.8.attention.self.value.weight', 'nncf_module.bert.encoder.layer.8.attention.self.value.bias', 'nncf_module.bert.encoder.layer.8.attention.self.value.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.8.attention.self.value.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.8.attention.self.value.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.8.attention.self.value.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.8.attention.self.value.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.8.attention.output.dense.weight', 'nncf_module.bert.encoder.layer.8.attention.output.dense.bias', 'nncf_module.bert.encoder.layer.8.attention.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.8.attention.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.8.attention.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.8.attention.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.8.attention.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.8.intermediate.dense.weight', 'nncf_module.bert.encoder.layer.8.intermediate.dense.bias', 'nncf_module.bert.encoder.layer.8.intermediate.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.8.intermediate.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.8.intermediate.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.8.intermediate.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.8.intermediate.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.8.output.dense.weight', 'nncf_module.bert.encoder.layer.8.output.dense.bias', 'nncf_module.bert.encoder.layer.8.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.8.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.8.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.8.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.8.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.8.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.8.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.9.attention.self.query.weight', 'nncf_module.bert.encoder.layer.9.attention.self.query.bias', 'nncf_module.bert.encoder.layer.9.attention.self.query.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.9.attention.self.query.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.9.attention.self.query.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.9.attention.self.query.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.9.attention.self.query.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.9.attention.self.key.weight', 'nncf_module.bert.encoder.layer.9.attention.self.key.bias', 'nncf_module.bert.encoder.layer.9.attention.self.key.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.9.attention.self.key.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.9.attention.self.key.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.9.attention.self.key.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.9.attention.self.key.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.9.attention.self.value.weight', 'nncf_module.bert.encoder.layer.9.attention.self.value.bias', 'nncf_module.bert.encoder.layer.9.attention.self.value.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.9.attention.self.value.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.9.attention.self.value.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.9.attention.self.value.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.9.attention.self.value.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.9.attention.output.dense.weight', 'nncf_module.bert.encoder.layer.9.attention.output.dense.bias', 'nncf_module.bert.encoder.layer.9.attention.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.9.attention.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.9.attention.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.9.attention.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.9.attention.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.9.intermediate.dense.weight', 'nncf_module.bert.encoder.layer.9.intermediate.dense.bias', 'nncf_module.bert.encoder.layer.9.intermediate.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.9.intermediate.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.9.intermediate.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.9.intermediate.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.9.intermediate.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.9.output.dense.weight', 'nncf_module.bert.encoder.layer.9.output.dense.bias', 'nncf_module.bert.encoder.layer.9.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.9.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.9.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.9.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.9.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.9.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.9.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.10.attention.self.query.weight', 'nncf_module.bert.encoder.layer.10.attention.self.query.bias', 'nncf_module.bert.encoder.layer.10.attention.self.query.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.10.attention.self.query.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.10.attention.self.query.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.10.attention.self.query.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.10.attention.self.query.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.10.attention.self.key.weight', 'nncf_module.bert.encoder.layer.10.attention.self.key.bias', 'nncf_module.bert.encoder.layer.10.attention.self.key.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.10.attention.self.key.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.10.attention.self.key.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.10.attention.self.key.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.10.attention.self.key.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.10.attention.self.value.weight', 'nncf_module.bert.encoder.layer.10.attention.self.value.bias', 'nncf_module.bert.encoder.layer.10.attention.self.value.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.10.attention.self.value.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.10.attention.self.value.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.10.attention.self.value.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.10.attention.self.value.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.10.attention.output.dense.weight', 'nncf_module.bert.encoder.layer.10.attention.output.dense.bias', 'nncf_module.bert.encoder.layer.10.attention.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.10.attention.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.10.attention.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.10.attention.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.10.attention.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.10.intermediate.dense.weight', 'nncf_module.bert.encoder.layer.10.intermediate.dense.bias', 'nncf_module.bert.encoder.layer.10.intermediate.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.10.intermediate.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.10.intermediate.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.10.intermediate.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.10.intermediate.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.10.output.dense.weight', 'nncf_module.bert.encoder.layer.10.output.dense.bias', 'nncf_module.bert.encoder.layer.10.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.10.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.10.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.10.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.10.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.10.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.10.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.11.attention.self.query.weight', 'nncf_module.bert.encoder.layer.11.attention.self.query.bias', 'nncf_module.bert.encoder.layer.11.attention.self.query.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.11.attention.self.query.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.11.attention.self.query.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.11.attention.self.query.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.11.attention.self.query.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.11.attention.self.key.weight', 'nncf_module.bert.encoder.layer.11.attention.self.key.bias', 'nncf_module.bert.encoder.layer.11.attention.self.key.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.11.attention.self.key.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.11.attention.self.key.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.11.attention.self.key.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.11.attention.self.key.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.11.attention.self.value.weight', 'nncf_module.bert.encoder.layer.11.attention.self.value.bias', 'nncf_module.bert.encoder.layer.11.attention.self.value.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.11.attention.self.value.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.11.attention.self.value.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.11.attention.self.value.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.11.attention.self.value.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.11.attention.output.dense.weight', 'nncf_module.bert.encoder.layer.11.attention.output.dense.bias', 'nncf_module.bert.encoder.layer.11.attention.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.11.attention.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.11.attention.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.11.attention.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.11.attention.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'nncf_module.bert.encoder.layer.11.intermediate.dense.weight', 'nncf_module.bert.encoder.layer.11.intermediate.dense.bias', 'nncf_module.bert.encoder.layer.11.intermediate.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.11.intermediate.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.11.intermediate.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.11.intermediate.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.11.intermediate.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.11.output.dense.weight', 'nncf_module.bert.encoder.layer.11.output.dense.bias', 'nncf_module.bert.encoder.layer.11.output.dense.pre_ops.0.op._binary_mask', 'nncf_module.bert.encoder.layer.11.output.dense.pre_ops.1.op._num_bits', 'nncf_module.bert.encoder.layer.11.output.dense.pre_ops.1.op.signed_tensor', 'nncf_module.bert.encoder.layer.11.output.dense.pre_ops.1.op.enabled', 'nncf_module.bert.encoder.layer.11.output.dense.pre_ops.1.op.scale', 'nncf_module.bert.encoder.layer.11.output.LayerNorm.weight', 'nncf_module.bert.encoder.layer.11.output.LayerNorm.bias', 'nncf_module.bert.pooler.dense.weight', 'nncf_module.bert.pooler.dense.bias', 'nncf_module.bert.pooler.dense.pre_ops.0.op._num_bits', 'nncf_module.bert.pooler.dense.pre_ops.0.op.signed_tensor', 'nncf_module.bert.pooler.dense.pre_ops.0.op.enabled', 'nncf_module.bert.pooler.dense.pre_ops.0.op.scale', 'nncf_module.classifier.weight', 'nncf_module.classifier.bias', 'nncf_module.classifier.pre_ops.0.op._num_bits', 'nncf_module.classifier.pre_ops.0.op.signed_tensor', 'nncf_module.classifier.pre_ops.0.op.enabled', 'nncf_module.classifier.pre_ops.0.op.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[6]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[7]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[8]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[key]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.signed_tensor', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[query]/linear_0|OUTPUT.scale', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/NNCFLinear[value]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertAttention[attention]/BertSelfAttention[self]/matmul_1|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertIntermediate[intermediate]/GELUActivation[intermediate_act_fn]/gelu_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0|INPUT0.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertPooler[pooler]/NNCFLinear[dense]/linear_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertPooler[pooler]/NNCFLinear[dense]/linear_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertPooler[pooler]/NNCFLinear[dense]/linear_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertPooler[pooler]/NNCFLinear[dense]/linear_0|OUTPUT.input_range', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertPooler[pooler]/Tanh[activation]/tanh_0|OUTPUT._num_bits', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertPooler[pooler]/Tanh[activation]/tanh_0|OUTPUT.input_low', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertPooler[pooler]/Tanh[activation]/tanh_0|OUTPUT.enabled', 'external_quantizers.BertForSequenceClassification/BertModel[bert]/BertPooler[pooler]/Tanh[activation]/tanh_0|OUTPUT.input_range']
2023-04-20 10:55:54,268 Model config
2023-04-20 10:55:56,330 Loading model models/GeneralDistilledModels/TinyBERT_General_6L_768D/pytorch_model.bin
2023-04-20 10:55:56,506 loading model...
2023-04-20 10:55:56,527 done!
2023-04-20 10:55:56,527 Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'fit_dense.weight', 'fit_dense.bias']
2023-04-20 10:55:56,527 Weights from pretrained model not used in TinyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias']
2023-04-20 10:55:56,528 ***** Running training *****
2023-04-20 10:55:56,529   Num examples = 1119160
2023-04-20 10:55:56,529   Batch size = 32
2023-04-20 10:55:56,529   Num steps = 34973
2023-04-20 10:55:56,530 n: bert.embeddings.word_embeddings.weight
2023-04-20 10:55:56,530 n: bert.embeddings.position_embeddings.weight
2023-04-20 10:55:56,530 n: bert.embeddings.token_type_embeddings.weight
2023-04-20 10:55:56,530 n: bert.embeddings.LayerNorm.weight
2023-04-20 10:55:56,530 n: bert.embeddings.LayerNorm.bias
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.attention.self.query.weight
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.attention.self.query.bias
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.attention.self.key.weight
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.attention.self.key.bias
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.attention.self.value.weight
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.attention.self.value.bias
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.attention.output.dense.weight
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.attention.output.dense.bias
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.intermediate.dense.weight
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.intermediate.dense.bias
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.output.dense.weight
2023-04-20 10:55:56,531 n: bert.encoder.layer.0.output.dense.bias
2023-04-20 10:55:56,532 n: bert.encoder.layer.0.output.LayerNorm.weight
2023-04-20 10:55:56,532 n: bert.encoder.layer.0.output.LayerNorm.bias
2023-04-20 10:55:56,532 n: bert.encoder.layer.1.attention.self.query.weight
2023-04-20 10:55:56,532 n: bert.encoder.layer.1.attention.self.query.bias
2023-04-20 10:55:56,532 n: bert.encoder.layer.1.attention.self.key.weight
2023-04-20 10:55:56,532 n: bert.encoder.layer.1.attention.self.key.bias
2023-04-20 10:55:56,532 n: bert.encoder.layer.1.attention.self.value.weight
2023-04-20 10:55:56,532 n: bert.encoder.layer.1.attention.self.value.bias
2023-04-20 10:55:56,532 n: bert.encoder.layer.1.attention.output.dense.weight
2023-04-20 10:55:56,532 n: bert.encoder.layer.1.attention.output.dense.bias
2023-04-20 10:55:56,532 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2023-04-20 10:55:56,532 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2023-04-20 10:55:56,532 n: bert.encoder.layer.1.intermediate.dense.weight
2023-04-20 10:55:56,533 n: bert.encoder.layer.1.intermediate.dense.bias
2023-04-20 10:55:56,533 n: bert.encoder.layer.1.output.dense.weight
2023-04-20 10:55:56,533 n: bert.encoder.layer.1.output.dense.bias
2023-04-20 10:55:56,533 n: bert.encoder.layer.1.output.LayerNorm.weight
2023-04-20 10:55:56,533 n: bert.encoder.layer.1.output.LayerNorm.bias
2023-04-20 10:55:56,533 n: bert.encoder.layer.2.attention.self.query.weight
2023-04-20 10:55:56,533 n: bert.encoder.layer.2.attention.self.query.bias
2023-04-20 10:55:56,533 n: bert.encoder.layer.2.attention.self.key.weight
2023-04-20 10:55:56,533 n: bert.encoder.layer.2.attention.self.key.bias
2023-04-20 10:55:56,533 n: bert.encoder.layer.2.attention.self.value.weight
2023-04-20 10:55:56,533 n: bert.encoder.layer.2.attention.self.value.bias
2023-04-20 10:55:56,533 n: bert.encoder.layer.2.attention.output.dense.weight
2023-04-20 10:55:56,533 n: bert.encoder.layer.2.attention.output.dense.bias
2023-04-20 10:55:56,533 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2023-04-20 10:55:56,534 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2023-04-20 10:55:56,534 n: bert.encoder.layer.2.intermediate.dense.weight
2023-04-20 10:55:56,534 n: bert.encoder.layer.2.intermediate.dense.bias
2023-04-20 10:55:56,534 n: bert.encoder.layer.2.output.dense.weight
2023-04-20 10:55:56,534 n: bert.encoder.layer.2.output.dense.bias
2023-04-20 10:55:56,534 n: bert.encoder.layer.2.output.LayerNorm.weight
2023-04-20 10:55:56,534 n: bert.encoder.layer.2.output.LayerNorm.bias
2023-04-20 10:55:56,534 n: bert.encoder.layer.3.attention.self.query.weight
2023-04-20 10:55:56,534 n: bert.encoder.layer.3.attention.self.query.bias
2023-04-20 10:55:56,534 n: bert.encoder.layer.3.attention.self.key.weight
2023-04-20 10:55:56,534 n: bert.encoder.layer.3.attention.self.key.bias
2023-04-20 10:55:56,534 n: bert.encoder.layer.3.attention.self.value.weight
2023-04-20 10:55:56,534 n: bert.encoder.layer.3.attention.self.value.bias
2023-04-20 10:55:56,534 n: bert.encoder.layer.3.attention.output.dense.weight
2023-04-20 10:55:56,535 n: bert.encoder.layer.3.attention.output.dense.bias
2023-04-20 10:55:56,535 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2023-04-20 10:55:56,535 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2023-04-20 10:55:56,535 n: bert.encoder.layer.3.intermediate.dense.weight
2023-04-20 10:55:56,535 n: bert.encoder.layer.3.intermediate.dense.bias
2023-04-20 10:55:56,535 n: bert.encoder.layer.3.output.dense.weight
2023-04-20 10:55:56,535 n: bert.encoder.layer.3.output.dense.bias
2023-04-20 10:55:56,535 n: bert.encoder.layer.3.output.LayerNorm.weight
2023-04-20 10:55:56,535 n: bert.encoder.layer.3.output.LayerNorm.bias
2023-04-20 10:55:56,535 n: bert.encoder.layer.4.attention.self.query.weight
2023-04-20 10:55:56,535 n: bert.encoder.layer.4.attention.self.query.bias
2023-04-20 10:55:56,535 n: bert.encoder.layer.4.attention.self.key.weight
2023-04-20 10:55:56,535 n: bert.encoder.layer.4.attention.self.key.bias
2023-04-20 10:55:56,535 n: bert.encoder.layer.4.attention.self.value.weight
2023-04-20 10:55:56,536 n: bert.encoder.layer.4.attention.self.value.bias
2023-04-20 10:55:56,536 n: bert.encoder.layer.4.attention.output.dense.weight
2023-04-20 10:55:56,536 n: bert.encoder.layer.4.attention.output.dense.bias
2023-04-20 10:55:56,536 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2023-04-20 10:55:56,536 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2023-04-20 10:55:56,536 n: bert.encoder.layer.4.intermediate.dense.weight
2023-04-20 10:55:56,536 n: bert.encoder.layer.4.intermediate.dense.bias
2023-04-20 10:55:56,536 n: bert.encoder.layer.4.output.dense.weight
2023-04-20 10:55:56,536 n: bert.encoder.layer.4.output.dense.bias
2023-04-20 10:55:56,536 n: bert.encoder.layer.4.output.LayerNorm.weight
2023-04-20 10:55:56,536 n: bert.encoder.layer.4.output.LayerNorm.bias
2023-04-20 10:55:56,536 n: bert.encoder.layer.5.attention.self.query.weight
2023-04-20 10:55:56,536 n: bert.encoder.layer.5.attention.self.query.bias
2023-04-20 10:55:56,536 n: bert.encoder.layer.5.attention.self.key.weight
2023-04-20 10:55:56,537 n: bert.encoder.layer.5.attention.self.key.bias
2023-04-20 10:55:56,537 n: bert.encoder.layer.5.attention.self.value.weight
2023-04-20 10:55:56,537 n: bert.encoder.layer.5.attention.self.value.bias
2023-04-20 10:55:56,537 n: bert.encoder.layer.5.attention.output.dense.weight
2023-04-20 10:55:56,537 n: bert.encoder.layer.5.attention.output.dense.bias
2023-04-20 10:55:56,537 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2023-04-20 10:55:56,537 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2023-04-20 10:55:56,537 n: bert.encoder.layer.5.intermediate.dense.weight
2023-04-20 10:55:56,537 n: bert.encoder.layer.5.intermediate.dense.bias
2023-04-20 10:55:56,537 n: bert.encoder.layer.5.output.dense.weight
2023-04-20 10:55:56,537 n: bert.encoder.layer.5.output.dense.bias
2023-04-20 10:55:56,537 n: bert.encoder.layer.5.output.LayerNorm.weight
2023-04-20 10:55:56,537 n: bert.encoder.layer.5.output.LayerNorm.bias
2023-04-20 10:55:56,538 n: bert.pooler.dense.weight
2023-04-20 10:55:56,538 n: bert.pooler.dense.bias
2023-04-20 10:55:56,538 n: classifier.weight
2023-04-20 10:55:56,538 n: classifier.bias
2023-04-20 10:55:56,538 n: fit_dense.weight
2023-04-20 10:55:56,538 n: fit_dense.bias
2023-04-20 10:55:56,538 Total parameters: 67547138
2023-04-20 10:56:58,875 ***** Running evaluation *****
2023-04-20 10:56:58,877   Epoch = 0 iter 4 step
2023-04-20 10:56:58,877   Num examples = 872
2023-04-20 10:56:58,877   Batch size = 32
2023-04-20 10:56:58,878 ***** Eval results *****
2023-04-20 10:56:58,878   att_loss = 1.9574383795261383
2023-04-20 10:56:58,878   cls_loss = 0.0
2023-04-20 10:56:58,878   global_step = 4
2023-04-20 10:56:58,878   loss = 9.129129767417908
2023-04-20 10:56:58,878   rep_loss = 7.171691298484802
2023-04-20 10:56:58,878 ***** Save model *****
2023-04-20 10:57:28,865 ***** Running evaluation *****
2023-04-20 10:57:28,866   Epoch = 0 iter 9 step
2023-04-20 10:57:28,866   Num examples = 872
2023-04-20 10:57:28,866   Batch size = 32
2023-04-20 10:57:28,868 ***** Eval results *****
2023-04-20 10:57:28,868   att_loss = 1.1869116061263614
2023-04-20 10:57:28,868   cls_loss = 0.0
2023-04-20 10:57:28,868   global_step = 9
2023-04-20 10:57:28,868   loss = 7.359515772925483
2023-04-20 10:57:28,868   rep_loss = 6.172604136996799
2023-04-20 10:57:28,868 ***** Save model *****
2023-04-20 10:57:58,564 ***** Running evaluation *****
2023-04-20 10:57:58,566   Epoch = 0 iter 14 step
2023-04-20 10:57:58,566   Num examples = 872
2023-04-20 10:57:58,566   Batch size = 32
2023-04-20 10:57:58,567 ***** Eval results *****
2023-04-20 10:57:58,567   att_loss = 0.8708885567528861
2023-04-20 10:57:58,568   cls_loss = 0.0
2023-04-20 10:57:58,568   global_step = 14
2023-04-20 10:57:58,568   loss = 6.334411008017404
2023-04-20 10:57:58,568   rep_loss = 5.4635224512645175
2023-04-20 10:57:58,568 ***** Save model *****
2023-04-20 10:58:27,764 ***** Running evaluation *****
2023-04-20 10:58:27,766   Epoch = 0 iter 19 step
2023-04-20 10:58:27,766   Num examples = 872
2023-04-20 10:58:27,766   Batch size = 32
2023-04-20 10:58:27,767 ***** Eval results *****
2023-04-20 10:58:27,767   att_loss = 0.6933082459788573
2023-04-20 10:58:27,767   cls_loss = 0.0
2023-04-20 10:58:27,767   global_step = 19
2023-04-20 10:58:27,767   loss = 5.7086031060469775
2023-04-20 10:58:27,767   rep_loss = 5.015294853009675
2023-04-20 10:58:27,768 ***** Save model *****
2023-04-20 10:58:55,693 ***** Running evaluation *****
2023-04-20 10:58:55,694   Epoch = 0 iter 24 step
2023-04-20 10:58:55,694   Num examples = 872
2023-04-20 10:58:55,694   Batch size = 32
2023-04-20 10:58:55,696 ***** Eval results *****
2023-04-20 10:58:55,696   att_loss = 0.5819268847505251
2023-04-20 10:58:55,696   cls_loss = 0.0
2023-04-20 10:58:55,696   global_step = 24
2023-04-20 10:58:55,696   loss = 5.295231064160665
2023-04-20 10:58:55,696   rep_loss = 4.713304171959559
2023-04-20 10:58:55,696 ***** Save model *****
2023-04-20 10:59:23,175 ***** Running evaluation *****
2023-04-20 10:59:23,176   Epoch = 0 iter 29 step
2023-04-20 10:59:23,177   Num examples = 872
2023-04-20 10:59:23,177   Batch size = 32
2023-04-20 10:59:23,178 ***** Eval results *****
2023-04-20 10:59:23,178   att_loss = 0.5066796952280505
2023-04-20 10:59:23,178   cls_loss = 0.0
2023-04-20 10:59:23,178   global_step = 29
2023-04-20 10:59:23,178   loss = 5.000550746917725
2023-04-20 10:59:23,178   rep_loss = 4.493871039357678
2023-04-20 10:59:23,178 ***** Save model *****
2023-04-20 11:05:35,288 ***** Running evaluation *****
2023-04-20 11:05:35,288   Epoch = 0 iter 34 step
2023-04-20 11:05:35,288   Num examples = 872
2023-04-20 11:05:35,289   Batch size = 32
2023-04-20 11:05:35,290 ***** Eval results *****
2023-04-20 11:05:35,290   att_loss = 0.4487819200491204
2023-04-20 11:05:35,290   cls_loss = 0.0
2023-04-20 11:05:35,290   global_step = 34
2023-04-20 11:05:35,290   loss = 4.7721908232745
2023-04-20 11:05:35,290   rep_loss = 4.323408898185281
2023-04-20 11:05:35,290 ***** Save model *****
2023-04-20 11:11:36,934 ***** Running evaluation *****
2023-04-20 11:11:36,935   Epoch = 0 iter 39 step
2023-04-20 11:11:36,935   Num examples = 872
2023-04-20 11:11:36,935   Batch size = 32
2023-04-20 11:11:36,936 ***** Eval results *****
2023-04-20 11:11:36,936   att_loss = 0.4060469953677593
2023-04-20 11:11:36,936   cls_loss = 0.0
2023-04-20 11:11:36,937   global_step = 39
2023-04-20 11:11:36,937   loss = 4.595800864390838
2023-04-20 11:11:36,937   rep_loss = 4.1897538625277
2023-04-20 11:11:36,937 ***** Save model *****
2023-04-20 13:28:40,376 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-uncased-sst2-int8-unstructured80', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, local_rank=-1, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-04-20 13:49:58,823 The args: Namespace(data_dir='data/data_augmentation/glue_data/SST-2', teacher_model='models/FineTunedModels/bert-base-uncased-sst2-int8-unstructured80', student_model='models/GeneralDistilledModels/TinyBERT_General_6L_768D', task_name='SST-2', output_dir='models/InterMedDistill/TestTinyBert', cache_dir='', max_seq_length=128, do_eval=False, do_lower_case=False, train_batch_size=32, eval_batch_size=32, learning_rate=5e-05, weight_decay=0.0001, num_train_epochs=1.0, warmup_proportion=0.1, no_cuda=True, local_rank=-1, seed=42, gradient_accumulation_steps=1, aug_train=True, eval_step=5, pred_distill=False, data_url='', temperature=1.0)
2023-05-29 14:57:42,000 *** Example ***
2023-05-29 14:57:42,000 guid: aug-1
2023-05-29 14:57:42,000 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-05-29 14:57:42,000 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 14:57:42,000 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 14:57:42,000 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 14:57:42,000 label: 0
2023-05-29 14:57:42,000 label_id: 0
2023-05-29 14:57:44,522 Converted and tokenised the training data.
2023-05-29 14:59:05,686 Writing example 0 of 1119160
2023-05-29 14:59:05,686 *** Example ***
2023-05-29 14:59:05,686 guid: aug-1
2023-05-29 14:59:05,686 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-05-29 14:59:05,686 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 14:59:05,687 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 14:59:05,687 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 14:59:05,687 label: 0
2023-05-29 14:59:05,687 label_id: 0
2023-05-29 14:59:06,477 Writing example 10000 of 1119160
2023-05-29 14:59:07,273 Writing example 20000 of 1119160
2023-05-29 14:59:08,025 Writing example 30000 of 1119160
2023-05-29 14:59:08,793 Writing example 40000 of 1119160
2023-05-29 14:59:09,548 Writing example 50000 of 1119160
2023-05-29 14:59:10,318 Writing example 60000 of 1119160
2023-05-29 14:59:11,075 Writing example 70000 of 1119160
2023-05-29 14:59:11,871 Writing example 80000 of 1119160
2023-05-29 14:59:12,663 Writing example 90000 of 1119160
2023-05-29 14:59:13,707 Writing example 100000 of 1119160
2023-05-29 14:59:14,455 Writing example 110000 of 1119160
2023-05-29 14:59:15,203 Writing example 120000 of 1119160
2023-05-29 14:59:15,969 Writing example 130000 of 1119160
2023-05-29 14:59:16,760 Writing example 140000 of 1119160
2023-05-29 14:59:17,536 Writing example 150000 of 1119160
2023-05-29 14:59:18,322 Writing example 160000 of 1119160
2023-05-29 14:59:19,154 Writing example 170000 of 1119160
2023-05-29 14:59:19,880 Writing example 180000 of 1119160
2023-05-29 14:59:21,000 Writing example 190000 of 1119160
2023-05-29 14:59:21,774 Writing example 200000 of 1119160
2023-05-29 14:59:22,565 Writing example 210000 of 1119160
2023-05-29 14:59:23,330 Writing example 220000 of 1119160
2023-05-29 14:59:24,152 Writing example 230000 of 1119160
2023-05-29 14:59:24,936 Writing example 240000 of 1119160
2023-05-29 14:59:25,743 Writing example 250000 of 1119160
2023-05-29 14:59:26,555 Writing example 260000 of 1119160
2023-05-29 14:59:27,346 Writing example 270000 of 1119160
2023-05-29 14:59:28,171 Writing example 280000 of 1119160
2023-05-29 14:59:28,911 Writing example 290000 of 1119160
2023-05-29 14:59:30,196 Writing example 300000 of 1119160
2023-05-29 14:59:30,966 Writing example 310000 of 1119160
2023-05-29 14:59:31,731 Writing example 320000 of 1119160
2023-05-29 14:59:32,498 Writing example 330000 of 1119160
2023-05-29 14:59:33,231 Writing example 340000 of 1119160
2023-05-29 14:59:33,990 Writing example 350000 of 1119160
2023-05-29 14:59:34,752 Writing example 360000 of 1119160
2023-05-29 14:59:35,685 Writing example 370000 of 1119160
2023-05-29 14:59:36,632 Writing example 380000 of 1119160
2023-05-29 14:59:37,519 Writing example 390000 of 1119160
2023-05-29 14:59:38,404 Writing example 400000 of 1119160
2023-05-29 14:59:39,287 Writing example 410000 of 1119160
2023-05-29 14:59:40,174 Writing example 420000 of 1119160
2023-05-29 14:59:41,108 Writing example 430000 of 1119160
2023-05-29 14:59:42,824 Writing example 440000 of 1119160
2023-05-29 14:59:43,770 Writing example 450000 of 1119160
2023-05-29 14:59:44,706 Writing example 460000 of 1119160
2023-05-29 14:59:45,666 Writing example 470000 of 1119160
2023-05-29 14:59:46,579 Writing example 480000 of 1119160
2023-05-29 14:59:47,458 Writing example 490000 of 1119160
2023-05-29 14:59:48,393 Writing example 500000 of 1119160
2023-05-29 14:59:49,317 Writing example 510000 of 1119160
2023-05-29 14:59:50,275 Writing example 520000 of 1119160
2023-05-29 14:59:51,221 Writing example 530000 of 1119160
2023-05-29 14:59:52,143 Writing example 540000 of 1119160
2023-05-29 14:59:53,116 Writing example 550000 of 1119160
2023-05-29 14:59:54,070 Writing example 560000 of 1119160
2023-05-29 14:59:55,030 Writing example 570000 of 1119160
2023-05-29 14:59:56,001 Writing example 580000 of 1119160
2023-05-29 14:59:56,905 Writing example 590000 of 1119160
2023-05-29 14:59:57,855 Writing example 600000 of 1119160
2023-05-29 14:59:58,810 Writing example 610000 of 1119160
2023-05-29 15:00:00,857 Writing example 620000 of 1119160
2023-05-29 15:00:01,742 Writing example 630000 of 1119160
2023-05-29 15:00:02,696 Writing example 640000 of 1119160
2023-05-29 15:00:03,616 Writing example 650000 of 1119160
2023-05-29 15:00:04,481 Writing example 660000 of 1119160
2023-05-29 15:00:05,411 Writing example 670000 of 1119160
2023-05-29 15:00:06,363 Writing example 680000 of 1119160
2023-05-29 15:00:07,203 Writing example 690000 of 1119160
2023-05-29 15:00:08,168 Writing example 700000 of 1119160
2023-05-29 15:00:09,120 Writing example 710000 of 1119160
2023-05-29 15:00:10,051 Writing example 720000 of 1119160
2023-05-29 15:00:10,989 Writing example 730000 of 1119160
2023-05-29 15:00:11,909 Writing example 740000 of 1119160
2023-05-29 15:00:12,861 Writing example 750000 of 1119160
2023-05-29 15:00:13,786 Writing example 760000 of 1119160
2023-05-29 15:00:14,667 Writing example 770000 of 1119160
2023-05-29 15:00:15,603 Writing example 780000 of 1119160
2023-05-29 15:00:16,534 Writing example 790000 of 1119160
2023-05-29 15:00:17,467 Writing example 800000 of 1119160
2023-05-29 15:00:18,444 Writing example 810000 of 1119160
2023-05-29 15:00:19,378 Writing example 820000 of 1119160
2023-05-29 15:00:20,369 Writing example 830000 of 1119160
2023-05-29 15:00:22,700 Writing example 840000 of 1119160
2023-05-29 15:00:23,639 Writing example 850000 of 1119160
2023-05-29 15:00:24,613 Writing example 860000 of 1119160
2023-05-29 15:00:25,547 Writing example 870000 of 1119160
2023-05-29 15:00:26,493 Writing example 880000 of 1119160
2023-05-29 15:00:27,390 Writing example 890000 of 1119160
2023-05-29 15:00:28,316 Writing example 900000 of 1119160
2023-05-29 15:00:29,232 Writing example 910000 of 1119160
2023-05-29 15:00:30,187 Writing example 920000 of 1119160
2023-05-29 15:00:31,110 Writing example 930000 of 1119160
2023-05-29 15:00:32,015 Writing example 940000 of 1119160
2023-05-29 15:00:32,932 Writing example 950000 of 1119160
2023-05-29 15:00:33,891 Writing example 960000 of 1119160
2023-05-29 15:00:34,826 Writing example 970000 of 1119160
2023-05-29 15:00:35,715 Writing example 980000 of 1119160
2023-05-29 15:00:36,617 Writing example 990000 of 1119160
2023-05-29 15:00:37,505 Writing example 1000000 of 1119160
2023-05-29 15:00:38,427 Writing example 1010000 of 1119160
2023-05-29 15:00:39,330 Writing example 1020000 of 1119160
2023-05-29 15:00:40,229 Writing example 1030000 of 1119160
2023-05-29 15:00:41,145 Writing example 1040000 of 1119160
2023-05-29 15:00:42,060 Writing example 1050000 of 1119160
2023-05-29 15:00:42,976 Writing example 1060000 of 1119160
2023-05-29 15:00:43,893 Writing example 1070000 of 1119160
2023-05-29 15:00:44,809 Writing example 1080000 of 1119160
2023-05-29 15:00:45,713 Writing example 1090000 of 1119160
2023-05-29 15:00:46,591 Writing example 1100000 of 1119160
2023-05-29 15:00:47,498 Writing example 1110000 of 1119160
2023-05-29 15:00:55,617 Converted and tokenised the training data.
2023-05-29 15:07:39,906 Writing example 0 of 1119160
2023-05-29 15:07:39,907 *** Example ***
2023-05-29 15:07:39,907 guid: aug-1
2023-05-29 15:07:39,907 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-05-29 15:07:39,907 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 15:07:39,907 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 15:07:39,907 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 15:07:39,907 label: 0
2023-05-29 15:07:39,907 label_id: 0
2023-05-29 15:07:42,984 Writing example 10000 of 1119160
2023-05-29 15:07:46,173 Writing example 20000 of 1119160
2023-05-29 15:07:49,157 Writing example 30000 of 1119160
2023-05-29 15:07:52,190 Writing example 40000 of 1119160
2023-05-29 15:07:55,190 Writing example 50000 of 1119160
2023-05-29 15:07:58,245 Writing example 60000 of 1119160
2023-05-29 15:08:01,250 Writing example 70000 of 1119160
2023-05-29 15:08:04,673 Writing example 80000 of 1119160
2023-05-29 15:08:07,830 Writing example 90000 of 1119160
2023-05-29 15:08:10,844 Writing example 100000 of 1119160
2023-05-29 15:08:13,773 Writing example 110000 of 1119160
2023-05-29 15:08:16,719 Writing example 120000 of 1119160
2023-05-29 15:08:19,782 Writing example 130000 of 1119160
2023-05-29 15:08:22,955 Writing example 140000 of 1119160
2023-05-29 15:08:26,031 Writing example 150000 of 1119160
2023-05-29 15:08:29,539 Writing example 160000 of 1119160
2023-05-29 15:08:32,855 Writing example 170000 of 1119160
2023-05-29 15:08:35,743 Writing example 180000 of 1119160
2023-05-29 15:08:38,681 Writing example 190000 of 1119160
2023-05-29 15:08:41,751 Writing example 200000 of 1119160
2023-05-29 15:08:44,853 Writing example 210000 of 1119160
2023-05-29 15:08:47,856 Writing example 220000 of 1119160
2023-05-29 15:08:51,089 Writing example 230000 of 1119160
2023-05-29 15:08:54,161 Writing example 240000 of 1119160
2023-05-29 15:08:57,312 Writing example 250000 of 1119160
2023-05-29 15:09:00,491 Writing example 260000 of 1119160
2023-05-29 15:09:04,131 Writing example 270000 of 1119160
2023-05-29 15:09:07,397 Writing example 280000 of 1119160
2023-05-29 15:09:10,331 Writing example 290000 of 1119160
2023-05-29 15:09:13,154 Writing example 300000 of 1119160
2023-05-29 15:09:16,181 Writing example 310000 of 1119160
2023-05-29 15:09:19,199 Writing example 320000 of 1119160
2023-05-29 15:09:22,153 Writing example 330000 of 1119160
2023-05-29 15:09:25,009 Writing example 340000 of 1119160
2023-05-29 15:09:27,986 Writing example 350000 of 1119160
2023-05-29 15:09:30,987 Writing example 360000 of 1119160
2023-05-29 15:09:34,099 Writing example 370000 of 1119160
2023-05-29 15:09:37,231 Writing example 380000 of 1119160
2023-05-29 15:09:40,194 Writing example 390000 of 1119160
2023-05-29 15:09:43,148 Writing example 400000 of 1119160
2023-05-29 15:09:46,886 Writing example 410000 of 1119160
2023-05-29 15:09:49,829 Writing example 420000 of 1119160
2023-05-29 15:09:52,920 Writing example 430000 of 1119160
2023-05-29 15:09:55,893 Writing example 440000 of 1119160
2023-05-29 15:09:59,043 Writing example 450000 of 1119160
2023-05-29 15:10:02,163 Writing example 460000 of 1119160
2023-05-29 15:10:12,215 *** Example ***
2023-05-29 15:10:12,215 guid: aug-1
2023-05-29 15:10:12,216 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-05-29 15:10:12,216 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 15:10:12,216 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 15:10:12,216 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 15:10:12,216 label: 0
2023-05-29 15:10:12,216 label_id: 0
2023-05-29 17:38:45,443 *** Example ***
2023-05-29 17:38:45,443 guid: aug-1
2023-05-29 17:38:45,443 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-05-29 17:38:45,444 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 17:38:45,444 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 17:38:45,444 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 17:38:45,444 label: 0
2023-05-29 17:38:45,444 label_id: 0
2023-05-29 18:19:12,332 Converted and tokenised the training data.
2023-05-29 18:19:20,087 *** Example ***
2023-05-29 18:19:20,088 guid: aug-1
2023-05-29 18:19:20,088 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-05-29 18:19:20,088 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:19:20,088 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:19:20,088 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:19:20,088 label: 0
2023-05-29 18:19:20,088 label_id: 0
2023-05-29 18:32:15,058 *** Example ***
2023-05-29 18:32:15,058 guid: aug-1
2023-05-29 18:32:15,058 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-05-29 18:32:15,059 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:32:15,059 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:32:15,059 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:32:15,059 label: 0
2023-05-29 18:32:15,059 label_id: 0
2023-05-29 18:32:34,720 Converted and tokenised the training data.
2023-05-29 18:36:50,520 *** Example ***
2023-05-29 18:36:50,520 guid: aug-1
2023-05-29 18:36:50,520 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-05-29 18:36:50,520 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:36:50,520 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:36:50,520 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:36:50,520 label: 0
2023-05-29 18:36:50,520 label_id: 0
2023-05-29 18:41:34,276 *** Example ***
2023-05-29 18:41:34,277 guid: aug-1
2023-05-29 18:41:34,277 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-05-29 18:41:34,277 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:41:34,277 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:41:34,277 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:41:34,277 label: 0
2023-05-29 18:41:34,277 label_id: 0
2023-05-29 18:43:56,199 Writing example 0 of 1119160
2023-05-29 18:43:56,200 *** Example ***
2023-05-29 18:43:56,200 guid: aug-1
2023-05-29 18:43:56,200 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-05-29 18:43:56,200 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:43:56,200 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:43:56,200 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 18:43:56,200 label: 0
2023-05-29 18:43:56,200 label_id: 0
2023-05-29 18:43:58,743 Writing example 10000 of 1119160
2023-05-29 18:44:01,350 Writing example 20000 of 1119160
2023-05-29 18:44:03,812 Writing example 30000 of 1119160
2023-05-29 18:44:06,317 Writing example 40000 of 1119160
2023-05-29 18:44:08,820 Writing example 50000 of 1119160
2023-05-29 18:44:11,353 Writing example 60000 of 1119160
2023-05-29 18:44:13,830 Writing example 70000 of 1119160
2023-05-29 18:44:16,679 Writing example 80000 of 1119160
2023-05-29 18:44:19,289 Writing example 90000 of 1119160
2023-05-29 18:44:21,791 Writing example 100000 of 1119160
2023-05-29 18:44:24,241 Writing example 110000 of 1119160
2023-05-29 18:44:26,696 Writing example 120000 of 1119160
2023-05-29 18:44:29,228 Writing example 130000 of 1119160
2023-05-29 18:44:31,856 Writing example 140000 of 1119160
2023-05-29 18:44:34,416 Writing example 150000 of 1119160
2023-05-29 18:44:37,372 Writing example 160000 of 1119160
2023-05-29 18:44:40,151 Writing example 170000 of 1119160
2023-05-29 18:44:42,603 Writing example 180000 of 1119160
2023-05-29 18:44:45,084 Writing example 190000 of 1119160
2023-05-29 18:44:47,682 Writing example 200000 of 1119160
2023-05-29 18:44:50,305 Writing example 210000 of 1119160
2023-05-29 18:44:52,877 Writing example 220000 of 1119160
2023-05-29 18:44:55,608 Writing example 230000 of 1119160
2023-05-29 18:44:58,197 Writing example 240000 of 1119160
2023-05-29 18:45:00,874 Writing example 250000 of 1119160
2023-05-29 18:45:03,614 Writing example 260000 of 1119160
2023-05-29 18:45:06,795 Writing example 270000 of 1119160
2023-05-29 18:45:09,559 Writing example 280000 of 1119160
2023-05-29 18:45:12,072 Writing example 290000 of 1119160
2023-05-29 18:45:14,514 Writing example 300000 of 1119160
2023-05-29 18:45:17,154 Writing example 310000 of 1119160
2023-05-29 18:45:19,708 Writing example 320000 of 1119160
2023-05-29 18:45:22,188 Writing example 330000 of 1119160
2023-05-29 18:45:24,573 Writing example 340000 of 1119160
2023-05-29 18:45:27,058 Writing example 350000 of 1119160
2023-05-29 18:45:29,562 Writing example 360000 of 1119160
2023-05-29 18:45:32,149 Writing example 370000 of 1119160
2023-05-29 18:45:34,749 Writing example 380000 of 1119160
2023-05-29 18:45:37,193 Writing example 390000 of 1119160
2023-05-29 18:45:39,641 Writing example 400000 of 1119160
2023-05-29 18:45:42,775 Writing example 410000 of 1119160
2023-05-29 18:45:45,210 Writing example 420000 of 1119160
2023-05-29 18:45:47,774 Writing example 430000 of 1119160
2023-05-29 18:45:50,211 Writing example 440000 of 1119160
2023-05-29 18:45:52,796 Writing example 450000 of 1119160
2023-05-29 18:45:55,359 Writing example 460000 of 1119160
2023-05-29 18:45:57,975 Writing example 470000 of 1119160
2023-05-29 18:46:00,470 Writing example 480000 of 1119160
2023-05-29 18:46:02,865 Writing example 490000 of 1119160
2023-05-29 18:46:05,365 Writing example 500000 of 1119160
2023-05-29 18:46:07,889 Writing example 510000 of 1119160
2023-05-29 18:46:10,602 Writing example 520000 of 1119160
2023-05-29 18:46:13,242 Writing example 530000 of 1119160
2023-05-29 18:46:15,810 Writing example 540000 of 1119160
2023-05-29 18:46:18,503 Writing example 550000 of 1119160
2023-05-29 18:46:21,185 Writing example 560000 of 1119160
2023-05-29 18:46:23,856 Writing example 570000 of 1119160
2023-05-29 18:46:27,510 Writing example 580000 of 1119160
2023-05-29 18:46:29,981 Writing example 590000 of 1119160
2023-05-29 18:46:32,586 Writing example 600000 of 1119160
2023-05-29 18:46:35,184 Writing example 610000 of 1119160
2023-05-29 18:46:37,772 Writing example 620000 of 1119160
2023-05-29 18:46:40,237 Writing example 630000 of 1119160
2023-05-29 18:46:42,907 Writing example 640000 of 1119160
2023-05-29 18:46:45,483 Writing example 650000 of 1119160
2023-05-29 18:46:47,870 Writing example 660000 of 1119160
2023-05-29 18:46:50,413 Writing example 670000 of 1119160
2023-05-29 18:46:53,009 Writing example 680000 of 1119160
2023-05-29 18:46:55,320 Writing example 690000 of 1119160
2023-05-29 18:46:57,989 Writing example 700000 of 1119160
2023-05-29 18:47:00,554 Writing example 710000 of 1119160
2023-05-29 18:47:03,069 Writing example 720000 of 1119160
2023-05-29 18:47:05,606 Writing example 730000 of 1119160
2023-05-29 18:47:08,097 Writing example 740000 of 1119160
2023-05-29 18:47:10,665 Writing example 750000 of 1119160
2023-05-29 18:47:13,208 Writing example 760000 of 1119160
2023-05-29 18:47:15,606 Writing example 770000 of 1119160
2023-05-29 18:47:18,101 Writing example 780000 of 1119160
2023-05-29 18:47:21,763 Writing example 790000 of 1119160
2023-05-29 18:47:24,260 Writing example 800000 of 1119160
2023-05-29 18:47:26,936 Writing example 810000 of 1119160
2023-05-29 18:47:29,464 Writing example 820000 of 1119160
2023-05-29 18:47:32,167 Writing example 830000 of 1119160
2023-05-29 18:47:34,607 Writing example 840000 of 1119160
2023-05-29 18:47:37,169 Writing example 850000 of 1119160
2023-05-29 18:47:39,837 Writing example 860000 of 1119160
2023-05-29 18:47:42,389 Writing example 870000 of 1119160
2023-05-29 18:47:44,987 Writing example 880000 of 1119160
2023-05-29 18:47:47,441 Writing example 890000 of 1119160
2023-05-29 18:47:49,953 Writing example 900000 of 1119160
2023-05-29 18:47:52,476 Writing example 910000 of 1119160
2023-05-29 18:47:55,083 Writing example 920000 of 1119160
2023-05-29 18:47:57,577 Writing example 930000 of 1119160
2023-05-29 18:48:00,062 Writing example 940000 of 1119160
2023-05-29 18:48:02,582 Writing example 950000 of 1119160
2023-05-29 18:48:05,222 Writing example 960000 of 1119160
2023-05-29 18:48:07,824 Writing example 970000 of 1119160
2023-05-29 18:48:10,299 Writing example 980000 of 1119160
2023-05-29 18:48:12,798 Writing example 990000 of 1119160
2023-05-29 18:48:15,255 Writing example 1000000 of 1119160
2023-05-29 18:48:17,801 Writing example 1010000 of 1119160
2023-05-29 18:48:20,293 Writing example 1020000 of 1119160
2023-05-29 18:48:22,779 Writing example 1030000 of 1119160
2023-05-29 18:48:25,304 Writing example 1040000 of 1119160
2023-05-29 18:48:29,436 Writing example 1050000 of 1119160
2023-05-29 18:48:31,948 Writing example 1060000 of 1119160
2023-05-29 18:48:34,474 Writing example 1070000 of 1119160
2023-05-29 18:48:37,013 Writing example 1080000 of 1119160
2023-05-29 18:48:39,530 Writing example 1090000 of 1119160
2023-05-29 18:48:41,912 Writing example 1100000 of 1119160
2023-05-29 18:48:44,402 Writing example 1110000 of 1119160
2023-05-29 18:48:56,997 Converted and tokenised the training data.
2023-05-29 21:38:03,172 Writing example 0 of 1119160
2023-05-29 21:38:03,173 *** Example ***
2023-05-29 21:38:03,173 guid: aug-1
2023-05-29 21:38:03,173 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2023-05-29 21:38:03,173 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 21:38:03,173 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 21:38:03,173 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2023-05-29 21:38:03,173 label: 0
2023-05-29 21:38:03,173 label_id: 0
2023-05-29 21:38:04,134 Writing example 10000 of 1119160
2023-05-29 21:38:05,093 Writing example 20000 of 1119160
2023-05-29 21:38:06,001 Writing example 30000 of 1119160
2023-05-29 21:38:06,920 Writing example 40000 of 1119160
2023-05-29 21:38:07,828 Writing example 50000 of 1119160
2023-05-29 21:38:08,751 Writing example 60000 of 1119160
2023-05-29 21:38:09,661 Writing example 70000 of 1119160
2023-05-29 21:38:10,614 Writing example 80000 of 1119160
2023-05-29 21:38:11,563 Writing example 90000 of 1119160
2023-05-29 21:38:12,780 Writing example 100000 of 1119160
2023-05-29 21:38:13,671 Writing example 110000 of 1119160
2023-05-29 21:38:14,563 Writing example 120000 of 1119160
2023-05-29 21:38:15,485 Writing example 130000 of 1119160
2023-05-29 21:38:16,440 Writing example 140000 of 1119160
2023-05-29 21:38:17,378 Writing example 150000 of 1119160
2023-05-29 21:38:18,326 Writing example 160000 of 1119160
2023-05-29 21:38:19,331 Writing example 170000 of 1119160
2023-05-29 21:38:20,211 Writing example 180000 of 1119160
2023-05-29 21:38:21,526 Writing example 190000 of 1119160
2023-05-29 21:38:22,458 Writing example 200000 of 1119160
2023-05-29 21:38:23,404 Writing example 210000 of 1119160
2023-05-29 21:38:24,315 Writing example 220000 of 1119160
2023-05-29 21:38:25,299 Writing example 230000 of 1119160
2023-05-29 21:38:26,231 Writing example 240000 of 1119160
2023-05-29 21:38:27,190 Writing example 250000 of 1119160
2023-05-29 21:38:28,152 Writing example 260000 of 1119160
2023-05-29 21:38:29,086 Writing example 270000 of 1119160
2023-05-29 21:38:30,075 Writing example 280000 of 1119160
2023-05-29 21:38:30,969 Writing example 290000 of 1119160
2023-05-29 21:38:32,419 Writing example 300000 of 1119160
2023-05-29 21:38:33,339 Writing example 310000 of 1119160
2023-05-29 21:38:34,259 Writing example 320000 of 1119160
2023-05-29 21:38:35,159 Writing example 330000 of 1119160
2023-05-29 21:38:36,026 Writing example 340000 of 1119160
2023-05-29 21:38:36,930 Writing example 350000 of 1119160
2023-05-29 21:38:37,842 Writing example 360000 of 1119160
2023-05-29 21:38:38,789 Writing example 370000 of 1119160
2023-05-29 21:38:39,740 Writing example 380000 of 1119160
2023-05-29 21:38:40,631 Writing example 390000 of 1119160
2023-05-29 21:38:41,521 Writing example 400000 of 1119160
2023-05-29 21:38:42,415 Writing example 410000 of 1119160
2023-05-29 21:38:43,302 Writing example 420000 of 1119160
2023-05-29 21:38:44,241 Writing example 430000 of 1119160
2023-05-29 21:38:45,939 Writing example 440000 of 1119160
2023-05-29 21:38:46,879 Writing example 450000 of 1119160
2023-05-29 21:38:47,814 Writing example 460000 of 1119160
2023-05-29 21:38:48,764 Writing example 470000 of 1119160
2023-05-29 21:38:49,676 Writing example 480000 of 1119160
2023-05-29 21:38:50,552 Writing example 490000 of 1119160
2023-05-29 21:38:51,461 Writing example 500000 of 1119160
2023-05-29 21:38:52,379 Writing example 510000 of 1119160
2023-05-29 21:38:53,338 Writing example 520000 of 1119160
2023-05-29 21:38:54,285 Writing example 530000 of 1119160
2023-05-29 21:38:55,211 Writing example 540000 of 1119160
2023-05-29 21:38:56,184 Writing example 550000 of 1119160
2023-05-29 21:38:57,138 Writing example 560000 of 1119160
2023-05-29 21:38:58,099 Writing example 570000 of 1119160
2023-05-29 21:38:59,068 Writing example 580000 of 1119160
2023-05-29 21:38:59,966 Writing example 590000 of 1119160
2023-05-29 21:39:00,907 Writing example 600000 of 1119160
2023-05-29 21:39:01,851 Writing example 610000 of 1119160
2023-05-29 21:39:03,845 Writing example 620000 of 1119160
2023-05-29 21:39:04,740 Writing example 630000 of 1119160
2023-05-29 21:39:05,701 Writing example 640000 of 1119160
2023-05-29 21:39:06,625 Writing example 650000 of 1119160
2023-05-29 21:39:07,497 Writing example 660000 of 1119160
2023-05-29 21:39:08,423 Writing example 670000 of 1119160
2023-05-29 21:39:09,368 Writing example 680000 of 1119160
2023-05-29 21:39:10,207 Writing example 690000 of 1119160
2023-05-29 21:39:11,163 Writing example 700000 of 1119160
2023-05-29 21:39:12,098 Writing example 710000 of 1119160
2023-05-29 21:39:13,019 Writing example 720000 of 1119160
2023-05-29 21:39:13,942 Writing example 730000 of 1119160
2023-05-29 21:39:14,852 Writing example 740000 of 1119160
2023-05-29 21:39:15,791 Writing example 750000 of 1119160
2023-05-29 21:39:16,718 Writing example 760000 of 1119160
2023-05-29 21:39:17,593 Writing example 770000 of 1119160
2023-05-29 21:39:18,505 Writing example 780000 of 1119160
2023-05-29 21:39:19,413 Writing example 790000 of 1119160
2023-05-29 21:39:20,328 Writing example 800000 of 1119160
2023-05-29 21:39:21,295 Writing example 810000 of 1119160
2023-05-29 21:39:22,218 Writing example 820000 of 1119160
2023-05-29 21:39:23,203 Writing example 830000 of 1119160
2023-05-29 21:39:25,488 Writing example 840000 of 1119160
2023-05-29 21:39:26,420 Writing example 850000 of 1119160
2023-05-29 21:39:27,394 Writing example 860000 of 1119160
2023-05-29 21:39:28,325 Writing example 870000 of 1119160
2023-05-29 21:39:29,271 Writing example 880000 of 1119160
2023-05-29 21:39:30,169 Writing example 890000 of 1119160
2023-05-29 21:39:31,089 Writing example 900000 of 1119160
2023-05-29 21:39:32,010 Writing example 910000 of 1119160
2023-05-29 21:39:32,965 Writing example 920000 of 1119160
2023-05-29 21:39:33,873 Writing example 930000 of 1119160
2023-05-29 21:39:34,780 Writing example 940000 of 1119160
2023-05-29 21:39:35,699 Writing example 950000 of 1119160
2023-05-29 21:39:36,661 Writing example 960000 of 1119160
2023-05-29 21:39:37,606 Writing example 970000 of 1119160
2023-05-29 21:39:38,507 Writing example 980000 of 1119160
2023-05-29 21:39:39,420 Writing example 990000 of 1119160
2023-05-29 21:39:40,319 Writing example 1000000 of 1119160
2023-05-29 21:39:41,251 Writing example 1010000 of 1119160
2023-05-29 21:39:42,159 Writing example 1020000 of 1119160
2023-05-29 21:39:43,067 Writing example 1030000 of 1119160
2023-05-29 21:39:43,991 Writing example 1040000 of 1119160
2023-05-29 21:39:44,911 Writing example 1050000 of 1119160
2023-05-29 21:39:45,827 Writing example 1060000 of 1119160
2023-05-29 21:39:46,747 Writing example 1070000 of 1119160
2023-05-29 21:39:47,667 Writing example 1080000 of 1119160
2023-05-29 21:39:48,580 Writing example 1090000 of 1119160
2023-05-29 21:39:49,451 Writing example 1100000 of 1119160
2023-05-29 21:39:50,362 Writing example 1110000 of 1119160
2023-05-29 21:40:04,761 Converted and tokenised the training data.
